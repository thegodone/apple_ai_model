{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e22d68fa-1fc1-444c-a98f-8920ed639da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlx.core as mx\n",
    "import math\n",
    "import mlx.optimizers as optim\n",
    "import mlx.nn as nn\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from featurer import save_smiles_dicts, get_smiles_dicts, get_smiles_array\n",
    "from attfp_mlx_utils import AttFP, cosineannealingwarmrestartfactor\n",
    "import psutil\n",
    "import cProfile\n",
    "import pstats\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e11633-6038-414c-8d54-4fcc485e1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 108 \n",
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "batch_size = 200\n",
    "epochs = 200\n",
    "p_dropout= 0.05\n",
    "fingerprint_dim = 192\n",
    "\n",
    "weight_decay = 5 # also known as l2_regularization_lambda\n",
    "learning_rate = 2.5\n",
    "output_units_num = 1 # for regression model\n",
    "radius = 2\n",
    "T = 2\n",
    "\n",
    "task_name = 'solubility'\n",
    "tasks = ['measured log solubility in mols per litre']\n",
    "raw_filename = \"delaney-processed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a35a8a-1f71-43c0-8002-06bc5afdb908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  1128\n",
      "number of successfully processed smiles:  1128\n",
      "not processed items\n"
     ]
    }
   ],
   "source": [
    "feature_filename = raw_filename.replace('.csv','.pickle')\n",
    "filename = raw_filename.replace('.csv','')\n",
    "prefix_filename = raw_filename.split('/')[-1].replace('.csv','')\n",
    "smiles_tasks_df = pd.read_csv(raw_filename)\n",
    "smilesList = smiles_tasks_df.smiles.values\n",
    "print(\"number of all smiles: \",len(smilesList))\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        remained_smiles.append(smiles)\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "    except:\n",
    "        print(smiles)\n",
    "        pass\n",
    "print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "smiles_tasks_df = smiles_tasks_df[smiles_tasks_df[\"smiles\"].isin(remained_smiles)]\n",
    "# print(smiles_tasks_df)\n",
    "smiles_tasks_df['cano_smiles'] =canonical_smiles_list\n",
    "\n",
    "if os.path.isfile(feature_filename):\n",
    "    feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "else:\n",
    "    feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "# feature_dicts = get_smiles_dicts(smilesList)\n",
    "remained_df = smiles_tasks_df[smiles_tasks_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = smiles_tasks_df.drop(remained_df.index)\n",
    "print(\"not processed items\")\n",
    "uncovered_df\n",
    "\n",
    "remained_df = remained_df.reset_index(drop=True)\n",
    "test_df = remained_df.sample(frac=1/10, random_state=random_seed) # test set\n",
    "training_data = remained_df.drop(test_df.index) # training data\n",
    "\n",
    "# training data is further divided into validation set and train set\n",
    "valid_df = training_data.sample(frac=1/9, random_state=random_seed) # validation set\n",
    "train_df = training_data.drop(valid_df.index) # train set\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# print(len(test_df),sorted(test_df.cano_smiles.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8609a3-6555-4372-8f45-41dc1d3e63c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105c635-c72d-4270-bdc4-dd0acf281524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df7db6bc-de0e-43c5-9a1b-9cebaea71b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    }
   ],
   "source": [
    "device = mx.gpu # or mx.cpu\n",
    "mx.set_default_device(device)\n",
    "\n",
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([canonical_smiles_list[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "x_atom = mx.array(x_atom)\n",
    "x_bonds = mx.array(x_bonds)\n",
    "x_atom_index = mx.array(x_atom_index)\n",
    "x_bond_index = mx.array(x_bond_index)\n",
    "x_mask = mx.array(x_mask)\n",
    "\n",
    "model = AttFP(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "\n",
    "print('init')\n",
    "def loss_fn(y_hat, y, parameters=None):\n",
    "    y = mx.reshape(y, y_hat.shape)\n",
    "    return mx.mean(nn.losses.mse_loss(y_hat, y))\n",
    "\n",
    "def forward_fn(model, x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, labels):\n",
    "    _, y_hat = model(x_atom, x_bonds, x_atom_index, x_bond_index, x_mask)\n",
    "    loss = loss_fn(y_hat, labels, model.parameters())\n",
    "    return loss, y_hat\n",
    "        \n",
    "def train(dataset,e, batch_size=64, doprofile=False):\n",
    "    print('train')\n",
    "    loss_sum = 0.0\n",
    "    np.random.seed(e)\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)   \n",
    "    #print('iter per batch:',len(batch_list))\n",
    "    if doprofile:\n",
    "        pr = cProfile.Profile()\n",
    "        pr.enable()  # Start profiling\n",
    "\n",
    "    for counter, train_batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[train_batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        y_val = mx.array(batch_df[tasks[0]].values)\n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom = mx.array(x_atom)\n",
    "        x_bonds = mx.array(x_bonds)\n",
    "        x_atom_index = mx.array(x_atom_index)\n",
    "        x_bond_index = mx.array(x_bond_index)\n",
    "        x_mask = mx.array(x_mask)\n",
    "\n",
    "        \n",
    "        (loss, y_hat), grads = loss_and_grad_fn(\n",
    "            model=model,\n",
    "            x_atom=x_atom,\n",
    "            x_bonds=x_bonds,\n",
    "            x_atom_index=x_atom_index,\n",
    "            x_bond_index=x_bond_index,\n",
    "            x_mask=x_mask,\n",
    "            labels=y_val,\n",
    "        )\n",
    "\n",
    "        optimizer.update(model, grads)\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "    if doprofile:\n",
    "        pr.disable()\n",
    "        # Print profiling results\n",
    "        s = io.StringIO()\n",
    "        ps = pstats.Stats(pr, stream=s).sort_stats(pstats.SortKey.CUMULATIVE)\n",
    "        ps.print_stats(200)  # Print top 10 results\n",
    "        print(s.getvalue())    \n",
    "    return loss_sum / len(dataset)\n",
    "\n",
    "\n",
    "\n",
    "def test(test_dataset, batch_size=64):\n",
    "    mse= 0.0\n",
    "    valList = np.arange(0,test_dataset.shape[0])\n",
    "    batch_list = []\n",
    "    for i in range(0, test_dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "        \n",
    "    for counter, test_batch in enumerate(batch_list):\n",
    "        batch_df = test_dataset.loc[test_batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, _ = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom = mx.array(x_atom)\n",
    "        x_bonds = mx.array(x_bonds)\n",
    "        x_atom_index = mx.array(x_atom_index)\n",
    "        x_bond_index = mx.array(x_bond_index)\n",
    "        x_mask = mx.array(x_mask)\n",
    "        y_val = mx.array(batch_df[tasks[0]].values)\n",
    "\n",
    "        _, y_hat = model(x_atom, x_bonds, x_atom_index, x_bond_index, x_mask)\n",
    "        y_val = mx.reshape(y_val, y_hat.shape)\n",
    "        mse += mx.square(y_hat - y_val).sum().item()\n",
    "        \n",
    "    val =  mse / len(test_dataset)\n",
    "    return val, np.sqrt(val)\n",
    "\n",
    "\n",
    "def epoch(e, batch_size=64):\n",
    "    loss = train(train_df,e, batch_size=batch_size)\n",
    "    train_mse, train_rmse = test(train_df, batch_size=2*batch_size)\n",
    "    valid_mse, valid_rmse = test(valid_df, batch_size=2*batch_size)\n",
    "\n",
    "    test_mse, test_rmse = test(test_df, batch_size=batch_size)\n",
    "    return loss, train_mse, train_rmse, valid_mse, valid_rmse,  test_mse, test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7ea774f-f526-4506-995e-8e4d684864d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950]\n",
      "200\n",
      "train\n",
      "Epoch:   0 | loss: 0.05354 | Train rmse: 5.224 | Val rmse: 5.381 | Test rmse: 4.752 | LR: 0.003113 | Time: 1.5946481227874756\n",
      "train\n",
      "Epoch:   1 | loss: 0.05868 | Train rmse: 2.005 | Val rmse: 1.911 | Test rmse: 1.812 | LR: 0.002916 | Time: 1.5627970695495605\n",
      "train\n",
      "Epoch:   2 | loss: 0.02278 | Train rmse: 1.784 | Val rmse: 1.733 | Test rmse: 1.437 | LR: 0.002589 | Time: 1.5697269439697266\n",
      "train\n",
      "Epoch:   3 | loss: 0.02073 | Train rmse: 1.721 | Val rmse: 1.671 | Test rmse: 1.517 | LR: 0.002163 | Time: 1.565659999847412\n",
      "train\n",
      "Epoch:   4 | loss: 0.01606 | Train rmse: 1.754 | Val rmse: 1.732 | Test rmse: 1.611 | LR: 0.001680 | Time: 1.5682897567749023\n",
      "train\n",
      "Epoch:   5 | loss: 0.01624 | Train rmse: 1.629 | Val rmse: 1.574 | Test rmse: 1.329 | LR: 0.001188 | Time: 1.5628290176391602\n",
      "train\n",
      "Epoch:   6 | loss: 0.01456 | Train rmse: 1.590 | Val rmse: 1.559 | Test rmse: 1.396 | LR: 0.000734 | Time: 1.575253963470459\n",
      "train\n",
      "Epoch:   7 | loss: 0.01384 | Train rmse: 1.583 | Val rmse: 1.539 | Test rmse: 1.334 | LR: 0.000363 | Time: 1.5745582580566406\n",
      "train\n",
      "Epoch:   8 | loss: 0.01398 | Train rmse: 1.573 | Val rmse: 1.524 | Test rmse: 1.331 | LR: 0.000111 | Time: 1.6070961952209473\n",
      "train\n",
      "Epoch:   9 | loss: 0.01361 | Train rmse: 1.554 | Val rmse: 1.526 | Test rmse: 1.342 | LR: 0.000003 | Time: 1.5785760879516602\n",
      "train\n",
      "Epoch:  10 | loss: 0.01517 | Train rmse: 1.517 | Val rmse: 1.515 | Test rmse: 1.298 | LR: 0.002957 | Time: 1.5632438659667969\n",
      "train\n",
      "Epoch:  11 | loss: 0.01362 | Train rmse: 1.520 | Val rmse: 1.543 | Test rmse: 1.302 | LR: 0.002770 | Time: 1.5749742984771729\n",
      "train\n",
      "Epoch:  12 | loss: 0.01231 | Train rmse: 1.417 | Val rmse: 1.492 | Test rmse: 1.285 | LR: 0.002460 | Time: 1.6517622470855713\n",
      "train\n",
      "Epoch:  13 | loss: 0.01128 | Train rmse: 1.336 | Val rmse: 1.440 | Test rmse: 1.246 | LR: 0.002055 | Time: 1.569638967514038\n",
      "train\n",
      "Epoch:  14 | loss: 0.01006 | Train rmse: 1.317 | Val rmse: 1.387 | Test rmse: 1.203 | LR: 0.001596 | Time: 1.5740149021148682\n",
      "train\n",
      "Epoch:  15 | loss: 0.01021 | Train rmse: 1.323 | Val rmse: 1.426 | Test rmse: 1.275 | LR: 0.001129 | Time: 1.5667510032653809\n",
      "train\n",
      "Epoch:  16 | loss: 0.00968 | Train rmse: 1.285 | Val rmse: 1.354 | Test rmse: 1.247 | LR: 0.000697 | Time: 1.5757989883422852\n",
      "train\n",
      "Epoch:  17 | loss: 0.00898 | Train rmse: 1.269 | Val rmse: 1.395 | Test rmse: 1.216 | LR: 0.000345 | Time: 1.572577953338623\n",
      "train\n",
      "Epoch:  18 | loss: 0.00900 | Train rmse: 1.262 | Val rmse: 1.329 | Test rmse: 1.209 | LR: 0.000105 | Time: 1.5715198516845703\n",
      "train\n",
      "Epoch:  19 | loss: 0.00861 | Train rmse: 1.248 | Val rmse: 1.336 | Test rmse: 1.232 | LR: 0.000003 | Time: 1.571882963180542\n",
      "train\n",
      "Epoch:  20 | loss: 0.00913 | Train rmse: 1.318 | Val rmse: 1.345 | Test rmse: 1.224 | LR: 0.002809 | Time: 1.5752010345458984\n",
      "train\n",
      "Epoch:  21 | loss: 0.00917 | Train rmse: 1.224 | Val rmse: 1.236 | Test rmse: 1.147 | LR: 0.002632 | Time: 1.5755479335784912\n",
      "train\n",
      "Epoch:  22 | loss: 0.00884 | Train rmse: 1.279 | Val rmse: 1.375 | Test rmse: 1.228 | LR: 0.002337 | Time: 1.5816938877105713\n",
      "train\n",
      "Epoch:  23 | loss: 0.00887 | Train rmse: 1.196 | Val rmse: 1.284 | Test rmse: 1.189 | LR: 0.001952 | Time: 1.581338882446289\n",
      "train\n",
      "Epoch:  24 | loss: 0.00834 | Train rmse: 1.181 | Val rmse: 1.200 | Test rmse: 1.133 | LR: 0.001517 | Time: 1.5799438953399658\n",
      "train\n",
      "Epoch:  25 | loss: 0.00801 | Train rmse: 1.173 | Val rmse: 1.239 | Test rmse: 1.144 | LR: 0.001072 | Time: 1.572329044342041\n",
      "train\n",
      "Epoch:  26 | loss: 0.00750 | Train rmse: 1.139 | Val rmse: 1.167 | Test rmse: 1.124 | LR: 0.000662 | Time: 1.574566125869751\n",
      "train\n",
      "Epoch:  27 | loss: 0.00706 | Train rmse: 1.110 | Val rmse: 1.177 | Test rmse: 1.121 | LR: 0.000327 | Time: 1.5753960609436035\n",
      "train\n",
      "Epoch:  28 | loss: 0.00691 | Train rmse: 1.100 | Val rmse: 1.158 | Test rmse: 1.125 | LR: 0.000100 | Time: 1.5806159973144531\n",
      "train\n",
      "Epoch:  29 | loss: 0.00663 | Train rmse: 1.103 | Val rmse: 1.175 | Test rmse: 1.123 | LR: 0.000003 | Time: 1.570152997970581\n",
      "train\n",
      "Epoch:  30 | loss: 0.00687 | Train rmse: 1.147 | Val rmse: 1.226 | Test rmse: 1.161 | LR: 0.002669 | Time: 1.5757691860198975\n",
      "train\n",
      "Epoch:  31 | loss: 0.00687 | Train rmse: 1.166 | Val rmse: 1.168 | Test rmse: 1.135 | LR: 0.002500 | Time: 1.5743811130523682\n",
      "train\n",
      "Epoch:  32 | loss: 0.00647 | Train rmse: 0.996 | Val rmse: 1.015 | Test rmse: 1.049 | LR: 0.002220 | Time: 1.5732998847961426\n",
      "train\n",
      "Epoch:  33 | loss: 0.00534 | Train rmse: 0.960 | Val rmse: 0.992 | Test rmse: 0.965 | LR: 0.001855 | Time: 1.5715689659118652\n",
      "train\n",
      "Epoch:  34 | loss: 0.00462 | Train rmse: 0.889 | Val rmse: 0.919 | Test rmse: 0.945 | LR: 0.001441 | Time: 1.570962905883789\n",
      "train\n",
      "Epoch:  35 | loss: 0.00449 | Train rmse: 0.863 | Val rmse: 0.898 | Test rmse: 0.878 | LR: 0.001018 | Time: 1.5788919925689697\n",
      "train\n",
      "Epoch:  36 | loss: 0.00406 | Train rmse: 0.841 | Val rmse: 0.880 | Test rmse: 0.864 | LR: 0.000629 | Time: 1.5842339992523193\n",
      "train\n",
      "Epoch:  37 | loss: 0.00393 | Train rmse: 0.831 | Val rmse: 0.909 | Test rmse: 0.859 | LR: 0.000311 | Time: 1.574833869934082\n",
      "train\n",
      "Epoch:  38 | loss: 0.00400 | Train rmse: 0.829 | Val rmse: 0.884 | Test rmse: 0.817 | LR: 0.000095 | Time: 1.5706629753112793\n",
      "train\n",
      "Epoch:  39 | loss: 0.00386 | Train rmse: 0.826 | Val rmse: 0.894 | Test rmse: 0.826 | LR: 0.000003 | Time: 1.5803899765014648\n",
      "train\n",
      "Epoch:  40 | loss: 0.00436 | Train rmse: 0.934 | Val rmse: 1.005 | Test rmse: 0.879 | LR: 0.002535 | Time: 1.5794098377227783\n",
      "train\n",
      "Epoch:  41 | loss: 0.00440 | Train rmse: 0.924 | Val rmse: 1.030 | Test rmse: 1.025 | LR: 0.002375 | Time: 1.6179468631744385\n",
      "train\n",
      "Epoch:  42 | loss: 0.00512 | Train rmse: 0.987 | Val rmse: 0.943 | Test rmse: 0.889 | LR: 0.002109 | Time: 1.581723928451538\n",
      "train\n",
      "Epoch:  43 | loss: 0.00471 | Train rmse: 0.854 | Val rmse: 0.919 | Test rmse: 0.881 | LR: 0.001762 | Time: 1.5740787982940674\n",
      "train\n",
      "Epoch:  44 | loss: 0.00397 | Train rmse: 0.824 | Val rmse: 0.869 | Test rmse: 0.850 | LR: 0.001369 | Time: 1.5707941055297852\n",
      "train\n",
      "Epoch:  45 | loss: 0.00402 | Train rmse: 0.816 | Val rmse: 0.881 | Test rmse: 0.878 | LR: 0.000968 | Time: 1.5707483291625977\n",
      "train\n",
      "Epoch:  46 | loss: 0.00358 | Train rmse: 0.765 | Val rmse: 0.849 | Test rmse: 0.820 | LR: 0.000598 | Time: 1.564857006072998\n",
      "train\n",
      "Epoch:  47 | loss: 0.00342 | Train rmse: 0.770 | Val rmse: 0.823 | Test rmse: 0.819 | LR: 0.000296 | Time: 1.5805799961090088\n",
      "train\n",
      "Epoch:  48 | loss: 0.00321 | Train rmse: 0.754 | Val rmse: 0.810 | Test rmse: 0.803 | LR: 0.000090 | Time: 1.5791409015655518\n",
      "train\n",
      "Epoch:  49 | loss: 0.00312 | Train rmse: 0.763 | Val rmse: 0.811 | Test rmse: 0.819 | LR: 0.000003 | Time: 1.576695203781128\n",
      "train\n",
      "Epoch:  50 | loss: 0.00395 | Train rmse: 0.776 | Val rmse: 0.873 | Test rmse: 0.813 | LR: 0.002408 | Time: 1.5681719779968262\n",
      "train\n",
      "Epoch:  51 | loss: 0.00356 | Train rmse: 0.759 | Val rmse: 0.884 | Test rmse: 0.869 | LR: 0.002256 | Time: 1.5694060325622559\n",
      "train\n",
      "Epoch:  52 | loss: 0.00327 | Train rmse: 0.759 | Val rmse: 0.853 | Test rmse: 0.814 | LR: 0.002003 | Time: 1.573218822479248\n",
      "train\n",
      "Epoch:  53 | loss: 0.00316 | Train rmse: 0.748 | Val rmse: 0.828 | Test rmse: 0.803 | LR: 0.001674 | Time: 1.5682780742645264\n",
      "train\n",
      "Epoch:  54 | loss: 0.00291 | Train rmse: 0.716 | Val rmse: 0.792 | Test rmse: 0.790 | LR: 0.001300 | Time: 1.5714330673217773\n",
      "train\n",
      "Epoch:  55 | loss: 0.00294 | Train rmse: 0.697 | Val rmse: 0.779 | Test rmse: 0.752 | LR: 0.000919 | Time: 1.5601389408111572\n",
      "train\n",
      "Epoch:  56 | loss: 0.00274 | Train rmse: 0.692 | Val rmse: 0.781 | Test rmse: 0.777 | LR: 0.000568 | Time: 1.5748860836029053\n",
      "train\n",
      "Epoch:  57 | loss: 0.00262 | Train rmse: 0.689 | Val rmse: 0.820 | Test rmse: 0.726 | LR: 0.000281 | Time: 1.5771667957305908\n",
      "train\n",
      "Epoch:  58 | loss: 0.00263 | Train rmse: 0.685 | Val rmse: 0.797 | Test rmse: 0.780 | LR: 0.000086 | Time: 1.5763611793518066\n",
      "train\n",
      "Epoch:  59 | loss: 0.00269 | Train rmse: 0.687 | Val rmse: 0.778 | Test rmse: 0.743 | LR: 0.000002 | Time: 1.5778932571411133\n",
      "train\n",
      "Epoch:  60 | loss: 0.00307 | Train rmse: 0.770 | Val rmse: 0.812 | Test rmse: 0.772 | LR: 0.002288 | Time: 1.5826797485351562\n",
      "train\n",
      "Epoch:  61 | loss: 0.00309 | Train rmse: 0.733 | Val rmse: 0.828 | Test rmse: 0.816 | LR: 0.002144 | Time: 1.5747120380401611\n",
      "train\n",
      "Epoch:  62 | loss: 0.00299 | Train rmse: 0.700 | Val rmse: 0.781 | Test rmse: 0.744 | LR: 0.001903 | Time: 1.5732741355895996\n",
      "train\n",
      "Epoch:  63 | loss: 0.00257 | Train rmse: 0.694 | Val rmse: 0.810 | Test rmse: 0.763 | LR: 0.001590 | Time: 1.57545804977417\n",
      "train\n",
      "Epoch:  64 | loss: 0.00266 | Train rmse: 0.664 | Val rmse: 0.789 | Test rmse: 0.720 | LR: 0.001235 | Time: 1.57588791847229\n",
      "train\n",
      "Epoch:  65 | loss: 0.00263 | Train rmse: 0.667 | Val rmse: 0.750 | Test rmse: 0.763 | LR: 0.000873 | Time: 1.571547031402588\n",
      "train\n",
      "Epoch:  66 | loss: 0.00261 | Train rmse: 0.651 | Val rmse: 0.776 | Test rmse: 0.733 | LR: 0.000540 | Time: 1.576828956604004\n",
      "train\n",
      "Epoch:  67 | loss: 0.00237 | Train rmse: 0.654 | Val rmse: 0.788 | Test rmse: 0.712 | LR: 0.000267 | Time: 1.5782229900360107\n",
      "train\n",
      "Epoch:  68 | loss: 0.00241 | Train rmse: 0.640 | Val rmse: 0.766 | Test rmse: 0.753 | LR: 0.000082 | Time: 1.582664966583252\n",
      "train\n",
      "Epoch:  69 | loss: 0.00227 | Train rmse: 0.650 | Val rmse: 0.794 | Test rmse: 0.739 | LR: 0.000002 | Time: 1.5772359371185303\n",
      "train\n",
      "Epoch:  70 | loss: 0.00274 | Train rmse: 0.688 | Val rmse: 0.737 | Test rmse: 0.755 | LR: 0.002174 | Time: 1.5793139934539795\n",
      "train\n",
      "Epoch:  71 | loss: 0.00276 | Train rmse: 0.769 | Val rmse: 0.889 | Test rmse: 0.893 | LR: 0.002036 | Time: 1.573828935623169\n",
      "train\n",
      "Epoch:  72 | loss: 0.00289 | Train rmse: 0.710 | Val rmse: 0.809 | Test rmse: 0.791 | LR: 0.001808 | Time: 1.5696690082550049\n",
      "train\n",
      "Epoch:  73 | loss: 0.00268 | Train rmse: 0.655 | Val rmse: 0.746 | Test rmse: 0.744 | LR: 0.001511 | Time: 1.5720949172973633\n",
      "train\n",
      "Epoch:  74 | loss: 0.00240 | Train rmse: 0.682 | Val rmse: 0.840 | Test rmse: 0.716 | LR: 0.001173 | Time: 1.5698890686035156\n",
      "train\n",
      "Epoch:  75 | loss: 0.00249 | Train rmse: 0.640 | Val rmse: 0.749 | Test rmse: 0.699 | LR: 0.000830 | Time: 1.5716609954833984\n",
      "train\n",
      "Epoch:  76 | loss: 0.00229 | Train rmse: 0.634 | Val rmse: 0.799 | Test rmse: 0.714 | LR: 0.000513 | Time: 1.62642502784729\n",
      "train\n",
      "Epoch:  77 | loss: 0.00215 | Train rmse: 0.621 | Val rmse: 0.739 | Test rmse: 0.706 | LR: 0.000253 | Time: 1.5833170413970947\n",
      "train\n",
      "Epoch:  78 | loss: 0.00215 | Train rmse: 0.616 | Val rmse: 0.739 | Test rmse: 0.704 | LR: 0.000078 | Time: 1.578981876373291\n",
      "train\n",
      "Epoch:  79 | loss: 0.00219 | Train rmse: 0.622 | Val rmse: 0.743 | Test rmse: 0.693 | LR: 0.000002 | Time: 1.5761229991912842\n",
      "train\n",
      "Epoch:  80 | loss: 0.00235 | Train rmse: 0.666 | Val rmse: 0.783 | Test rmse: 0.745 | LR: 0.002065 | Time: 1.5804407596588135\n",
      "train\n",
      "Epoch:  81 | loss: 0.00238 | Train rmse: 0.691 | Val rmse: 0.825 | Test rmse: 0.762 | LR: 0.001935 | Time: 1.5794751644134521\n",
      "train\n",
      "Epoch:  82 | loss: 0.00252 | Train rmse: 0.621 | Val rmse: 0.766 | Test rmse: 0.695 | LR: 0.001718 | Time: 1.5768709182739258\n",
      "train\n",
      "Epoch:  83 | loss: 0.00264 | Train rmse: 0.662 | Val rmse: 0.777 | Test rmse: 0.691 | LR: 0.001435 | Time: 1.5821797847747803\n",
      "train\n",
      "Epoch:  84 | loss: 0.00248 | Train rmse: 0.657 | Val rmse: 0.714 | Test rmse: 0.769 | LR: 0.001115 | Time: 1.5819759368896484\n",
      "train\n",
      "Epoch:  85 | loss: 0.00241 | Train rmse: 0.612 | Val rmse: 0.718 | Test rmse: 0.685 | LR: 0.000788 | Time: 1.5708110332489014\n",
      "train\n",
      "Epoch:  86 | loss: 0.00229 | Train rmse: 0.618 | Val rmse: 0.772 | Test rmse: 0.665 | LR: 0.000487 | Time: 1.574087142944336\n",
      "train\n",
      "Epoch:  87 | loss: 0.00209 | Train rmse: 0.593 | Val rmse: 0.745 | Test rmse: 0.690 | LR: 0.000241 | Time: 1.5734329223632812\n",
      "train\n",
      "Epoch:  88 | loss: 0.00204 | Train rmse: 0.598 | Val rmse: 0.784 | Test rmse: 0.687 | LR: 0.000074 | Time: 1.5676970481872559\n",
      "train\n",
      "Epoch:  89 | loss: 0.00195 | Train rmse: 0.598 | Val rmse: 0.784 | Test rmse: 0.671 | LR: 0.000002 | Time: 1.5739309787750244\n",
      "train\n",
      "Epoch:  90 | loss: 0.00258 | Train rmse: 0.720 | Val rmse: 0.803 | Test rmse: 0.822 | LR: 0.001962 | Time: 1.5811741352081299\n",
      "train\n",
      "Epoch:  91 | loss: 0.00297 | Train rmse: 0.775 | Val rmse: 0.933 | Test rmse: 0.837 | LR: 0.001838 | Time: 1.5749790668487549\n",
      "train\n",
      "Epoch:  92 | loss: 0.00275 | Train rmse: 0.637 | Val rmse: 0.784 | Test rmse: 0.705 | LR: 0.001632 | Time: 1.5727627277374268\n",
      "train\n",
      "Epoch:  93 | loss: 0.00218 | Train rmse: 0.623 | Val rmse: 0.735 | Test rmse: 0.690 | LR: 0.001363 | Time: 1.5870039463043213\n",
      "train\n",
      "Epoch:  94 | loss: 0.00222 | Train rmse: 0.609 | Val rmse: 0.744 | Test rmse: 0.661 | LR: 0.001059 | Time: 1.576923131942749\n",
      "train\n",
      "Epoch:  95 | loss: 0.00195 | Train rmse: 0.590 | Val rmse: 0.782 | Test rmse: 0.675 | LR: 0.000749 | Time: 1.571767807006836\n",
      "train\n",
      "Epoch:  96 | loss: 0.00192 | Train rmse: 0.608 | Val rmse: 0.770 | Test rmse: 0.673 | LR: 0.000463 | Time: 1.577186107635498\n",
      "train\n",
      "Epoch:  97 | loss: 0.00196 | Train rmse: 0.585 | Val rmse: 0.765 | Test rmse: 0.675 | LR: 0.000229 | Time: 1.5979819297790527\n",
      "train\n",
      "Epoch:  98 | loss: 0.00198 | Train rmse: 0.587 | Val rmse: 0.734 | Test rmse: 0.649 | LR: 0.000070 | Time: 1.5698127746582031\n",
      "train\n",
      "Epoch:  99 | loss: 0.00184 | Train rmse: 0.565 | Val rmse: 0.729 | Test rmse: 0.675 | LR: 0.000002 | Time: 1.5797419548034668\n",
      "train\n",
      "Epoch: 100 | loss: 0.00236 | Train rmse: 0.694 | Val rmse: 0.767 | Test rmse: 0.713 | LR: 0.001864 | Time: 1.5698251724243164\n",
      "train\n",
      "Epoch: 101 | loss: 0.00260 | Train rmse: 0.616 | Val rmse: 0.752 | Test rmse: 0.750 | LR: 0.001746 | Time: 1.569573163986206\n",
      "train\n",
      "Epoch: 102 | loss: 0.00221 | Train rmse: 0.613 | Val rmse: 0.752 | Test rmse: 0.655 | LR: 0.001550 | Time: 1.5700280666351318\n",
      "train\n",
      "Epoch: 103 | loss: 0.00201 | Train rmse: 0.590 | Val rmse: 0.703 | Test rmse: 0.682 | LR: 0.001295 | Time: 1.5806210041046143\n",
      "train\n",
      "Epoch: 104 | loss: 0.00196 | Train rmse: 0.607 | Val rmse: 0.698 | Test rmse: 0.636 | LR: 0.001006 | Time: 1.576019048690796\n",
      "train\n",
      "Epoch: 105 | loss: 0.00206 | Train rmse: 0.570 | Val rmse: 0.720 | Test rmse: 0.642 | LR: 0.000711 | Time: 1.5744829177856445\n",
      "train\n",
      "Epoch: 106 | loss: 0.00182 | Train rmse: 0.571 | Val rmse: 0.734 | Test rmse: 0.664 | LR: 0.000439 | Time: 1.588257074356079\n",
      "train\n",
      "Epoch: 107 | loss: 0.00174 | Train rmse: 0.577 | Val rmse: 0.744 | Test rmse: 0.644 | LR: 0.000217 | Time: 1.5831749439239502\n",
      "train\n",
      "Epoch: 108 | loss: 0.00178 | Train rmse: 0.560 | Val rmse: 0.757 | Test rmse: 0.636 | LR: 0.000066 | Time: 1.5727508068084717\n",
      "train\n",
      "Epoch: 109 | loss: 0.00176 | Train rmse: 0.556 | Val rmse: 0.716 | Test rmse: 0.651 | LR: 0.000002 | Time: 1.5803101062774658\n",
      "train\n",
      "Epoch: 110 | loss: 0.00219 | Train rmse: 0.581 | Val rmse: 0.763 | Test rmse: 0.642 | LR: 0.001770 | Time: 1.5820238590240479\n",
      "train\n",
      "Epoch: 111 | loss: 0.00216 | Train rmse: 0.686 | Val rmse: 0.821 | Test rmse: 0.809 | LR: 0.001659 | Time: 1.5731348991394043\n",
      "train\n",
      "Epoch: 112 | loss: 0.00230 | Train rmse: 0.653 | Val rmse: 0.703 | Test rmse: 0.663 | LR: 0.001473 | Time: 1.5816650390625\n",
      "train\n",
      "Epoch: 113 | loss: 0.00241 | Train rmse: 0.588 | Val rmse: 0.783 | Test rmse: 0.703 | LR: 0.001230 | Time: 1.568174123764038\n",
      "train\n",
      "Epoch: 114 | loss: 0.00236 | Train rmse: 0.602 | Val rmse: 0.725 | Test rmse: 0.632 | LR: 0.000956 | Time: 1.569777011871338\n",
      "train\n",
      "Epoch: 115 | loss: 0.00212 | Train rmse: 0.606 | Val rmse: 0.743 | Test rmse: 0.673 | LR: 0.000676 | Time: 1.5817689895629883\n",
      "train\n",
      "Epoch: 116 | loss: 0.00191 | Train rmse: 0.605 | Val rmse: 0.784 | Test rmse: 0.714 | LR: 0.000417 | Time: 1.5758821964263916\n",
      "train\n",
      "Epoch: 117 | loss: 0.00186 | Train rmse: 0.568 | Val rmse: 0.724 | Test rmse: 0.651 | LR: 0.000206 | Time: 1.5704550743103027\n",
      "train\n",
      "Epoch: 118 | loss: 0.00176 | Train rmse: 0.555 | Val rmse: 0.733 | Test rmse: 0.623 | LR: 0.000063 | Time: 1.5724380016326904\n",
      "train\n",
      "Epoch: 119 | loss: 0.00169 | Train rmse: 0.563 | Val rmse: 0.708 | Test rmse: 0.623 | LR: 0.000002 | Time: 1.577035903930664\n",
      "train\n",
      "Epoch: 120 | loss: 0.00211 | Train rmse: 0.641 | Val rmse: 0.826 | Test rmse: 0.744 | LR: 0.001682 | Time: 1.5812580585479736\n",
      "train\n",
      "Epoch: 121 | loss: 0.00221 | Train rmse: 0.561 | Val rmse: 0.715 | Test rmse: 0.639 | LR: 0.001576 | Time: 1.5708701610565186\n",
      "train\n",
      "Epoch: 122 | loss: 0.00173 | Train rmse: 0.571 | Val rmse: 0.739 | Test rmse: 0.654 | LR: 0.001399 | Time: 1.5868842601776123\n",
      "train\n",
      "Epoch: 123 | loss: 0.00186 | Train rmse: 0.545 | Val rmse: 0.719 | Test rmse: 0.630 | LR: 0.001169 | Time: 1.5760929584503174\n",
      "train\n",
      "Epoch: 124 | loss: 0.00184 | Train rmse: 0.548 | Val rmse: 0.723 | Test rmse: 0.649 | LR: 0.000908 | Time: 1.5791401863098145\n",
      "train\n",
      "Epoch: 125 | loss: 0.00169 | Train rmse: 0.537 | Val rmse: 0.674 | Test rmse: 0.654 | LR: 0.000642 | Time: 1.5788099765777588\n",
      "train\n",
      "Epoch: 126 | loss: 0.00155 | Train rmse: 0.543 | Val rmse: 0.724 | Test rmse: 0.635 | LR: 0.000397 | Time: 1.5751230716705322\n",
      "train\n",
      "Epoch: 127 | loss: 0.00159 | Train rmse: 0.529 | Val rmse: 0.733 | Test rmse: 0.628 | LR: 0.000196 | Time: 1.5717501640319824\n",
      "train\n",
      "Epoch: 128 | loss: 0.00162 | Train rmse: 0.533 | Val rmse: 0.702 | Test rmse: 0.631 | LR: 0.000060 | Time: 1.5682077407836914\n",
      "train\n",
      "Epoch: 129 | loss: 0.00164 | Train rmse: 0.533 | Val rmse: 0.723 | Test rmse: 0.590 | LR: 0.000002 | Time: 1.5683369636535645\n",
      "train\n",
      "Epoch: 130 | loss: 0.00170 | Train rmse: 0.553 | Val rmse: 0.710 | Test rmse: 0.650 | LR: 0.001598 | Time: 1.5776028633117676\n",
      "train\n",
      "Epoch: 131 | loss: 0.00163 | Train rmse: 0.544 | Val rmse: 0.700 | Test rmse: 0.649 | LR: 0.001497 | Time: 1.5698401927947998\n",
      "train\n",
      "Epoch: 132 | loss: 0.00189 | Train rmse: 0.591 | Val rmse: 0.695 | Test rmse: 0.644 | LR: 0.001329 | Time: 1.5751512050628662\n",
      "train\n",
      "Epoch: 133 | loss: 0.00193 | Train rmse: 0.641 | Val rmse: 0.794 | Test rmse: 0.715 | LR: 0.001110 | Time: 1.5727019309997559\n",
      "train\n",
      "Epoch: 134 | loss: 0.00190 | Train rmse: 0.576 | Val rmse: 0.776 | Test rmse: 0.647 | LR: 0.000863 | Time: 1.5691850185394287\n",
      "train\n",
      "Epoch: 135 | loss: 0.00181 | Train rmse: 0.546 | Val rmse: 0.699 | Test rmse: 0.583 | LR: 0.000610 | Time: 1.5756280422210693\n",
      "train\n",
      "Epoch: 136 | loss: 0.00163 | Train rmse: 0.546 | Val rmse: 0.714 | Test rmse: 0.632 | LR: 0.000377 | Time: 1.5799219608306885\n",
      "train\n",
      "Epoch: 137 | loss: 0.00158 | Train rmse: 0.535 | Val rmse: 0.709 | Test rmse: 0.624 | LR: 0.000186 | Time: 1.5751399993896484\n",
      "train\n",
      "Epoch: 138 | loss: 0.00154 | Train rmse: 0.519 | Val rmse: 0.698 | Test rmse: 0.603 | LR: 0.000057 | Time: 1.5759928226470947\n",
      "train\n",
      "Epoch: 139 | loss: 0.00160 | Train rmse: 0.525 | Val rmse: 0.717 | Test rmse: 0.608 | LR: 0.000002 | Time: 1.6437568664550781\n",
      "train\n",
      "Epoch: 140 | loss: 0.00175 | Train rmse: 0.562 | Val rmse: 0.715 | Test rmse: 0.624 | LR: 0.001518 | Time: 1.5798232555389404\n",
      "train\n",
      "Epoch: 141 | loss: 0.00165 | Train rmse: 0.628 | Val rmse: 0.828 | Test rmse: 0.707 | LR: 0.001422 | Time: 1.5705180168151855\n",
      "train\n",
      "Epoch: 142 | loss: 0.00235 | Train rmse: 0.578 | Val rmse: 0.693 | Test rmse: 0.640 | LR: 0.001263 | Time: 1.5747599601745605\n",
      "train\n",
      "Epoch: 143 | loss: 0.00209 | Train rmse: 0.620 | Val rmse: 0.727 | Test rmse: 0.682 | LR: 0.001055 | Time: 1.575937032699585\n",
      "train\n",
      "Epoch: 144 | loss: 0.00196 | Train rmse: 0.535 | Val rmse: 0.705 | Test rmse: 0.635 | LR: 0.000819 | Time: 1.5750458240509033\n",
      "train\n",
      "Epoch: 145 | loss: 0.00174 | Train rmse: 0.536 | Val rmse: 0.711 | Test rmse: 0.630 | LR: 0.000579 | Time: 1.5745289325714111\n",
      "train\n",
      "Epoch: 146 | loss: 0.00163 | Train rmse: 0.519 | Val rmse: 0.697 | Test rmse: 0.625 | LR: 0.000358 | Time: 1.5744969844818115\n",
      "train\n",
      "Epoch: 147 | loss: 0.00150 | Train rmse: 0.514 | Val rmse: 0.690 | Test rmse: 0.588 | LR: 0.000177 | Time: 1.5734436511993408\n",
      "train\n",
      "Epoch: 148 | loss: 0.00152 | Train rmse: 0.526 | Val rmse: 0.693 | Test rmse: 0.614 | LR: 0.000054 | Time: 1.5804219245910645\n",
      "train\n",
      "Epoch: 149 | loss: 0.00148 | Train rmse: 0.527 | Val rmse: 0.680 | Test rmse: 0.627 | LR: 0.000002 | Time: 1.5740411281585693\n",
      "train\n",
      "Epoch: 150 | loss: 0.00159 | Train rmse: 0.562 | Val rmse: 0.757 | Test rmse: 0.619 | LR: 0.001442 | Time: 1.5720281600952148\n",
      "train\n",
      "Epoch: 151 | loss: 0.00177 | Train rmse: 0.568 | Val rmse: 0.696 | Test rmse: 0.650 | LR: 0.001351 | Time: 1.5758640766143799\n",
      "train\n",
      "Epoch: 152 | loss: 0.00175 | Train rmse: 0.576 | Val rmse: 0.763 | Test rmse: 0.654 | LR: 0.001199 | Time: 1.58197021484375\n",
      "train\n",
      "Epoch: 153 | loss: 0.00169 | Train rmse: 0.575 | Val rmse: 0.750 | Test rmse: 0.648 | LR: 0.001002 | Time: 1.5765130519866943\n",
      "train\n",
      "Epoch: 154 | loss: 0.00164 | Train rmse: 0.548 | Val rmse: 0.751 | Test rmse: 0.622 | LR: 0.000779 | Time: 1.5703070163726807\n",
      "train\n",
      "Epoch: 155 | loss: 0.00166 | Train rmse: 0.518 | Val rmse: 0.671 | Test rmse: 0.605 | LR: 0.000550 | Time: 1.5855519771575928\n",
      "train\n",
      "Epoch: 156 | loss: 0.00153 | Train rmse: 0.513 | Val rmse: 0.674 | Test rmse: 0.580 | LR: 0.000340 | Time: 1.576287031173706\n",
      "train\n",
      "Epoch: 157 | loss: 0.00146 | Train rmse: 0.511 | Val rmse: 0.694 | Test rmse: 0.619 | LR: 0.000168 | Time: 1.5737197399139404\n",
      "train\n",
      "Epoch: 158 | loss: 0.00151 | Train rmse: 0.518 | Val rmse: 0.707 | Test rmse: 0.575 | LR: 0.000051 | Time: 1.5694317817687988\n",
      "train\n",
      "Epoch: 159 | loss: 0.00136 | Train rmse: 0.508 | Val rmse: 0.714 | Test rmse: 0.601 | LR: 0.000001 | Time: 1.5819740295410156\n",
      "train\n",
      "Epoch: 160 | loss: 0.00172 | Train rmse: 0.552 | Val rmse: 0.693 | Test rmse: 0.654 | LR: 0.001370 | Time: 1.5907471179962158\n",
      "train\n",
      "Epoch: 161 | loss: 0.00156 | Train rmse: 0.584 | Val rmse: 0.813 | Test rmse: 0.653 | LR: 0.001283 | Time: 1.5728631019592285\n",
      "train\n",
      "Epoch: 162 | loss: 0.00173 | Train rmse: 0.530 | Val rmse: 0.661 | Test rmse: 0.661 | LR: 0.001139 | Time: 1.575078010559082\n",
      "train\n",
      "Epoch: 163 | loss: 0.00151 | Train rmse: 0.523 | Val rmse: 0.654 | Test rmse: 0.631 | LR: 0.000952 | Time: 1.5878257751464844\n",
      "train\n",
      "Epoch: 164 | loss: 0.00157 | Train rmse: 0.516 | Val rmse: 0.671 | Test rmse: 0.625 | LR: 0.000740 | Time: 1.5753021240234375\n",
      "train\n",
      "Epoch: 165 | loss: 0.00156 | Train rmse: 0.510 | Val rmse: 0.678 | Test rmse: 0.616 | LR: 0.000523 | Time: 1.5730080604553223\n",
      "train\n",
      "Epoch: 166 | loss: 0.00144 | Train rmse: 0.503 | Val rmse: 0.706 | Test rmse: 0.561 | LR: 0.000323 | Time: 1.576965093612671\n",
      "train\n",
      "Epoch: 167 | loss: 0.00138 | Train rmse: 0.501 | Val rmse: 0.689 | Test rmse: 0.591 | LR: 0.000160 | Time: 1.5717248916625977\n",
      "train\n",
      "Epoch: 168 | loss: 0.00142 | Train rmse: 0.493 | Val rmse: 0.704 | Test rmse: 0.574 | LR: 0.000049 | Time: 1.5753018856048584\n",
      "train\n",
      "Epoch: 169 | loss: 0.00136 | Train rmse: 0.491 | Val rmse: 0.689 | Test rmse: 0.578 | LR: 0.000001 | Time: 1.5826501846313477\n",
      "train\n",
      "Epoch: 170 | loss: 0.00296 | Train rmse: 0.794 | Val rmse: 0.857 | Test rmse: 0.789 | LR: 0.001301 | Time: 1.5785481929779053\n",
      "train\n",
      "Epoch: 171 | loss: 0.00294 | Train rmse: 0.584 | Val rmse: 0.721 | Test rmse: 0.685 | LR: 0.001219 | Time: 1.5704717636108398\n",
      "train\n",
      "Epoch: 172 | loss: 0.00226 | Train rmse: 0.587 | Val rmse: 0.680 | Test rmse: 0.658 | LR: 0.001083 | Time: 1.5756840705871582\n",
      "train\n",
      "Epoch: 173 | loss: 0.00208 | Train rmse: 0.544 | Val rmse: 0.681 | Test rmse: 0.646 | LR: 0.000904 | Time: 1.5837371349334717\n",
      "train\n",
      "Epoch: 174 | loss: 0.00188 | Train rmse: 0.570 | Val rmse: 0.687 | Test rmse: 0.634 | LR: 0.000703 | Time: 1.5707249641418457\n",
      "train\n",
      "Epoch: 175 | loss: 0.00166 | Train rmse: 0.514 | Val rmse: 0.658 | Test rmse: 0.567 | LR: 0.000497 | Time: 1.5798871517181396\n",
      "train\n",
      "Epoch: 176 | loss: 0.00149 | Train rmse: 0.509 | Val rmse: 0.689 | Test rmse: 0.615 | LR: 0.000307 | Time: 1.5805718898773193\n",
      "train\n",
      "Epoch: 177 | loss: 0.00142 | Train rmse: 0.507 | Val rmse: 0.683 | Test rmse: 0.621 | LR: 0.000152 | Time: 1.569869041442871\n",
      "train\n",
      "Epoch: 178 | loss: 0.00144 | Train rmse: 0.510 | Val rmse: 0.672 | Test rmse: 0.600 | LR: 0.000046 | Time: 1.5794012546539307\n",
      "train\n",
      "Epoch: 179 | loss: 0.00146 | Train rmse: 0.508 | Val rmse: 0.671 | Test rmse: 0.645 | LR: 0.000001 | Time: 1.5736589431762695\n",
      "train\n",
      "Epoch: 180 | loss: 0.00166 | Train rmse: 0.504 | Val rmse: 0.703 | Test rmse: 0.596 | LR: 0.001236 | Time: 1.590665340423584\n",
      "train\n",
      "Epoch: 181 | loss: 0.00147 | Train rmse: 0.503 | Val rmse: 0.671 | Test rmse: 0.576 | LR: 0.001158 | Time: 1.5795090198516846\n",
      "train\n",
      "Epoch: 182 | loss: 0.00151 | Train rmse: 0.501 | Val rmse: 0.655 | Test rmse: 0.577 | LR: 0.001028 | Time: 1.5903007984161377\n",
      "train\n",
      "Epoch: 183 | loss: 0.00142 | Train rmse: 0.505 | Val rmse: 0.713 | Test rmse: 0.579 | LR: 0.000859 | Time: 1.5694701671600342\n",
      "train\n",
      "Epoch: 184 | loss: 0.00146 | Train rmse: 0.494 | Val rmse: 0.704 | Test rmse: 0.588 | LR: 0.000667 | Time: 1.576340913772583\n",
      "train\n",
      "Epoch: 185 | loss: 0.00134 | Train rmse: 0.490 | Val rmse: 0.684 | Test rmse: 0.598 | LR: 0.000472 | Time: 1.5727496147155762\n",
      "train\n",
      "Epoch: 186 | loss: 0.00135 | Train rmse: 0.491 | Val rmse: 0.668 | Test rmse: 0.612 | LR: 0.000292 | Time: 1.5699398517608643\n",
      "train\n",
      "Epoch: 187 | loss: 0.00130 | Train rmse: 0.478 | Val rmse: 0.682 | Test rmse: 0.569 | LR: 0.000144 | Time: 1.5706839561462402\n",
      "train\n",
      "Epoch: 188 | loss: 0.00132 | Train rmse: 0.485 | Val rmse: 0.662 | Test rmse: 0.592 | LR: 0.000044 | Time: 1.5715680122375488\n",
      "train\n",
      "Epoch: 189 | loss: 0.00126 | Train rmse: 0.492 | Val rmse: 0.682 | Test rmse: 0.560 | LR: 0.000001 | Time: 1.5726397037506104\n",
      "train\n",
      "Epoch: 190 | loss: 0.00152 | Train rmse: 0.498 | Val rmse: 0.669 | Test rmse: 0.570 | LR: 0.001175 | Time: 1.5683467388153076\n",
      "train\n",
      "Epoch: 191 | loss: 0.00150 | Train rmse: 0.496 | Val rmse: 0.689 | Test rmse: 0.561 | LR: 0.001100 | Time: 1.5762500762939453\n",
      "train\n",
      "Epoch: 192 | loss: 0.00167 | Train rmse: 0.588 | Val rmse: 0.769 | Test rmse: 0.658 | LR: 0.000977 | Time: 1.5783371925354004\n",
      "train\n",
      "Epoch: 193 | loss: 0.00156 | Train rmse: 0.519 | Val rmse: 0.718 | Test rmse: 0.605 | LR: 0.000816 | Time: 1.5844511985778809\n",
      "train\n",
      "Epoch: 194 | loss: 0.00143 | Train rmse: 0.503 | Val rmse: 0.725 | Test rmse: 0.596 | LR: 0.000634 | Time: 1.5804626941680908\n",
      "train\n",
      "Epoch: 195 | loss: 0.00135 | Train rmse: 0.495 | Val rmse: 0.664 | Test rmse: 0.607 | LR: 0.000448 | Time: 1.584275245666504\n",
      "train\n",
      "Epoch: 196 | loss: 0.00135 | Train rmse: 0.478 | Val rmse: 0.692 | Test rmse: 0.568 | LR: 0.000277 | Time: 1.5740270614624023\n",
      "train\n",
      "Epoch: 197 | loss: 0.00134 | Train rmse: 0.476 | Val rmse: 0.651 | Test rmse: 0.584 | LR: 0.000137 | Time: 1.5757830142974854\n",
      "train\n",
      "Epoch: 198 | loss: 0.00128 | Train rmse: 0.475 | Val rmse: 0.682 | Test rmse: 0.558 | LR: 0.000042 | Time: 1.5819599628448486\n",
      "train\n",
      "Epoch: 199 | loss: 0.00128 | Train rmse: 0.468 | Val rmse: 0.678 | Test rmse: 0.591 | LR: 0.000001 | Time: 1.579319715499878\n",
      "\n",
      "==> Best test mse: 0.311,  rmse: 0.558\n"
     ]
    }
   ],
   "source": [
    "mx.eval(model.parameters())\n",
    "\n",
    "\n",
    "# Example: Dynamically create learning rate schedules based on milestones\n",
    "initial_lr = 10**-learning_rate\n",
    "restarts = 20\n",
    "decay_step = 10*5  # Decay steps for each cosine and warmup phase\n",
    "warmup_factor = 0.95  # Warmup reduction factors\n",
    "\n",
    "\n",
    "lr_schedule = cosineannealingwarmrestartfactor(initial_lr, restarts, decay_step, warmup_factor)\n",
    "\n",
    "optimizer = optim.AdamW(learning_rate=lr_schedule, weight_decay=1**-weight_decay)\n",
    "loss_and_grad_fn = nn.value_and_grad(model, forward_fn)\n",
    "\n",
    "r = []\n",
    "best_test_mse = 1e9\n",
    "print(batch_size)\n",
    "for e in range(epochs):\n",
    "    starttime = time.time()\n",
    "    \n",
    "    loss, train_mse, train_rmse, valid_mse, valid_rmse, test_mse, test_rmse = epoch(e, batch_size=batch_size)\n",
    "    stoptime = time.time()\n",
    "\n",
    "    #print('RAM memory % used:', psutil.virtual_memory()[2],'RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "\n",
    "    best_test_mse = min(best_test_mse, test_mse)\n",
    "    r.append((train_rmse,test_rmse))\n",
    "    print(\n",
    "        \" | \".join(\n",
    "            [\n",
    "                f\"Epoch: {e:3d}\",\n",
    "                f\"loss: {loss:.5f}\",\n",
    "                f\"Train rmse: {train_rmse:.3f}\",\n",
    "                f\"Val rmse: {valid_rmse:.3f}\",\n",
    "                f\"Test rmse: {test_rmse:.3f}\",\n",
    "                f\"LR: {np.array(optimizer.learning_rate):.6f}\",\n",
    "                f\"Time: {stoptime-starttime}\",\n",
    "\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "print(f\"\\n==> Best test mse: {best_test_mse:.3f},  rmse: {np.sqrt(best_test_mse):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c533e9f-0336-473b-a7c5-aac864ced158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6068f09b-5cec-4335-82e0-79e3d34cb14d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c5ce7-f8ac-4128-9396-b4a24c38cdde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ded4ab7-bc8d-4469-8c24-ebe50c1683e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7e245-6c99-4b61-a762-4a75e3365fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
