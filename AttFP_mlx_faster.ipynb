{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e22d68fa-1fc1-444c-a98f-8920ed639da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlx.core as mx\n",
    "import math\n",
    "import mlx.optimizers as optim\n",
    "import mlx.nn as nn\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from featurer import save_smiles_dicts, get_smiles_dicts, get_smiles_array\n",
    "from attfp_mlx_utils import AttFP, cosineannealingwarmrestartfactor\n",
    "import psutil\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e11633-6038-414c-8d54-4fcc485e1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 108 \n",
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "batch_size = 200\n",
    "epochs = 200\n",
    "p_dropout= 0.05\n",
    "fingerprint_dim = 192\n",
    "\n",
    "weight_decay = 5 # also known as l2_regularization_lambda\n",
    "learning_rate = 2.5\n",
    "output_units_num = 1 # for regression model\n",
    "radius = 2\n",
    "T = 2\n",
    "\n",
    "task_name = 'solubility'\n",
    "tasks = ['measured log solubility in mols per litre']\n",
    "raw_filename = \"delaney-processed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a35a8a-1f71-43c0-8002-06bc5afdb908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  1128\n",
      "number of successfully processed smiles:  1128\n",
      "not processed items\n"
     ]
    }
   ],
   "source": [
    "feature_filename = raw_filename.replace('.csv','.pickle')\n",
    "filename = raw_filename.replace('.csv','')\n",
    "prefix_filename = raw_filename.split('/')[-1].replace('.csv','')\n",
    "smiles_tasks_df = pd.read_csv(raw_filename)\n",
    "smilesList = smiles_tasks_df.smiles.values\n",
    "print(\"number of all smiles: \",len(smilesList))\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        remained_smiles.append(smiles)\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "    except:\n",
    "        print(smiles)\n",
    "        pass\n",
    "print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "smiles_tasks_df = smiles_tasks_df[smiles_tasks_df[\"smiles\"].isin(remained_smiles)]\n",
    "# print(smiles_tasks_df)\n",
    "smiles_tasks_df['cano_smiles'] =canonical_smiles_list\n",
    "\n",
    "if os.path.isfile(feature_filename):\n",
    "    feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "else:\n",
    "    feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "# feature_dicts = get_smiles_dicts(smilesList)\n",
    "remained_df = smiles_tasks_df[smiles_tasks_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = smiles_tasks_df.drop(remained_df.index)\n",
    "print(\"not processed items\")\n",
    "uncovered_df\n",
    "\n",
    "remained_df = remained_df.reset_index(drop=True)\n",
    "test_df = remained_df.sample(frac=1/10, random_state=random_seed) # test set\n",
    "training_data = remained_df.drop(test_df.index) # training data\n",
    "\n",
    "# training data is further divided into validation set and train set\n",
    "valid_df = training_data.sample(frac=1/9, random_state=random_seed) # validation set\n",
    "train_df = training_data.drop(valid_df.index) # train set\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# print(len(test_df),sorted(test_df.cano_smiles.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40181f00-e309-45bd-9b17-282adcf8c4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950]\n",
      "200\n",
      "train\n",
      "Epoch:   0 | loss: 0.00000 | rmse: 2.347 | rmse: 2.392 | rmse: 1.966 | LR: 0.003113 | Time: 0.7618570327758789\n",
      "train\n",
      "Epoch:   1 | loss: 0.00000 | rmse: 2.042 | rmse: 2.033 | rmse: 1.742 | LR: 0.002916 | Time: 0.6949000358581543\n",
      "train\n",
      "Epoch:   2 | loss: 0.00000 | rmse: 1.855 | rmse: 1.845 | rmse: 1.685 | LR: 0.002589 | Time: 0.6866519451141357\n",
      "train\n",
      "Epoch:   3 | loss: 0.00000 | rmse: 1.845 | rmse: 1.794 | rmse: 1.551 | LR: 0.002163 | Time: 0.6845080852508545\n",
      "train\n",
      "Epoch:   4 | loss: 0.00000 | rmse: 1.952 | rmse: 1.963 | rmse: 1.722 | LR: 0.001680 | Time: 0.6807301044464111\n",
      "train\n",
      "Epoch:   5 | loss: 0.00000 | rmse: 1.615 | rmse: 1.631 | rmse: 1.455 | LR: 0.001188 | Time: 0.6799459457397461\n",
      "train\n",
      "Epoch:   6 | loss: 0.00000 | rmse: 1.472 | rmse: 1.531 | rmse: 1.429 | LR: 0.000734 | Time: 0.6814389228820801\n",
      "train\n",
      "Epoch:   7 | loss: 0.00000 | rmse: 1.424 | rmse: 1.471 | rmse: 1.336 | LR: 0.000363 | Time: 0.6834580898284912\n",
      "train\n",
      "Epoch:   8 | loss: 0.00000 | rmse: 1.402 | rmse: 1.473 | rmse: 1.343 | LR: 0.000111 | Time: 0.6848077774047852\n",
      "train\n",
      "Epoch:   9 | loss: 0.00000 | rmse: 1.394 | rmse: 1.458 | rmse: 1.329 | LR: 0.000003 | Time: 0.6868801116943359\n",
      "train\n",
      "Epoch:  10 | loss: 0.00000 | rmse: 1.455 | rmse: 1.515 | rmse: 1.327 | LR: 0.002957 | Time: 0.687204122543335\n",
      "train\n",
      "Epoch:  11 | loss: 0.00000 | rmse: 1.493 | rmse: 1.573 | rmse: 1.453 | LR: 0.002770 | Time: 0.6846561431884766\n",
      "train\n",
      "Epoch:  12 | loss: 0.00000 | rmse: 1.276 | rmse: 1.365 | rmse: 1.218 | LR: 0.002460 | Time: 0.6877999305725098\n",
      "train\n",
      "Epoch:  13 | loss: 0.00000 | rmse: 1.206 | rmse: 1.259 | rmse: 1.182 | LR: 0.002055 | Time: 0.6842758655548096\n",
      "train\n",
      "Epoch:  14 | loss: 0.00000 | rmse: 1.214 | rmse: 1.198 | rmse: 1.179 | LR: 0.001596 | Time: 0.6815879344940186\n",
      "train\n",
      "Epoch:  15 | loss: 0.00000 | rmse: 1.167 | rmse: 1.211 | rmse: 1.207 | LR: 0.001129 | Time: 0.6807558536529541\n",
      "train\n",
      "Epoch:  16 | loss: 0.00000 | rmse: 1.143 | rmse: 1.217 | rmse: 1.191 | LR: 0.000697 | Time: 0.6814758777618408\n",
      "train\n",
      "Epoch:  17 | loss: 0.00000 | rmse: 1.107 | rmse: 1.115 | rmse: 1.128 | LR: 0.000345 | Time: 0.6790251731872559\n",
      "train\n",
      "Epoch:  18 | loss: 0.00000 | rmse: 1.109 | rmse: 1.106 | rmse: 1.125 | LR: 0.000105 | Time: 0.6810929775238037\n",
      "train\n",
      "Epoch:  19 | loss: 0.00000 | rmse: 1.100 | rmse: 1.107 | rmse: 1.126 | LR: 0.000003 | Time: 0.6811869144439697\n",
      "train\n",
      "Epoch:  20 | loss: 0.00000 | rmse: 1.370 | rmse: 1.419 | rmse: 1.410 | LR: 0.002809 | Time: 0.6906123161315918\n",
      "train\n",
      "Epoch:  21 | loss: 0.00000 | rmse: 1.155 | rmse: 1.278 | rmse: 1.163 | LR: 0.002632 | Time: 0.6947660446166992\n",
      "train\n",
      "Epoch:  22 | loss: 0.00000 | rmse: 1.081 | rmse: 1.146 | rmse: 1.154 | LR: 0.002337 | Time: 0.6955239772796631\n",
      "train\n",
      "Epoch:  23 | loss: 0.00000 | rmse: 0.992 | rmse: 1.048 | rmse: 1.061 | LR: 0.001952 | Time: 0.6882607936859131\n",
      "train\n",
      "Epoch:  24 | loss: 0.00000 | rmse: 0.975 | rmse: 0.980 | rmse: 1.018 | LR: 0.001517 | Time: 0.6860611438751221\n",
      "train\n",
      "Epoch:  25 | loss: 0.00000 | rmse: 0.935 | rmse: 0.948 | rmse: 0.977 | LR: 0.001072 | Time: 0.6862380504608154\n",
      "train\n",
      "Epoch:  26 | loss: 0.00000 | rmse: 0.884 | rmse: 0.927 | rmse: 0.963 | LR: 0.000662 | Time: 0.6857700347900391\n",
      "train\n",
      "Epoch:  27 | loss: 0.00000 | rmse: 0.868 | rmse: 0.894 | rmse: 0.943 | LR: 0.000327 | Time: 0.6882729530334473\n",
      "train\n",
      "Epoch:  28 | loss: 0.00000 | rmse: 0.859 | rmse: 0.896 | rmse: 0.943 | LR: 0.000100 | Time: 0.6837937831878662\n",
      "train\n",
      "Epoch:  29 | loss: 0.00000 | rmse: 0.858 | rmse: 0.897 | rmse: 0.943 | LR: 0.000003 | Time: 0.68296217918396\n",
      "train\n",
      "Epoch:  30 | loss: 0.00000 | rmse: 0.818 | rmse: 0.868 | rmse: 0.845 | LR: 0.002669 | Time: 0.6822001934051514\n",
      "train\n",
      "Epoch:  31 | loss: 0.00000 | rmse: 0.792 | rmse: 0.831 | rmse: 0.790 | LR: 0.002500 | Time: 0.6807138919830322\n",
      "train\n",
      "Epoch:  32 | loss: 0.00000 | rmse: 0.797 | rmse: 0.858 | rmse: 0.810 | LR: 0.002220 | Time: 0.6804778575897217\n",
      "train\n",
      "Epoch:  33 | loss: 0.00000 | rmse: 0.816 | rmse: 0.889 | rmse: 0.849 | LR: 0.001855 | Time: 0.6787781715393066\n",
      "train\n",
      "Epoch:  34 | loss: 0.00000 | rmse: 0.761 | rmse: 0.798 | rmse: 0.800 | LR: 0.001441 | Time: 0.680156946182251\n",
      "train\n",
      "Epoch:  35 | loss: 0.00000 | rmse: 0.753 | rmse: 0.761 | rmse: 0.755 | LR: 0.001018 | Time: 0.6803081035614014\n",
      "train\n",
      "Epoch:  36 | loss: 0.00000 | rmse: 0.718 | rmse: 0.774 | rmse: 0.737 | LR: 0.000629 | Time: 0.6778848171234131\n",
      "train\n",
      "Epoch:  37 | loss: 0.00000 | rmse: 0.709 | rmse: 0.743 | rmse: 0.723 | LR: 0.000311 | Time: 0.6773028373718262\n",
      "train\n",
      "Epoch:  38 | loss: 0.00000 | rmse: 0.707 | rmse: 0.757 | rmse: 0.727 | LR: 0.000095 | Time: 0.6801018714904785\n",
      "train\n",
      "Epoch:  39 | loss: 0.00000 | rmse: 0.707 | rmse: 0.757 | rmse: 0.727 | LR: 0.000003 | Time: 0.6804759502410889\n",
      "train\n",
      "Epoch:  40 | loss: 0.00000 | rmse: 0.742 | rmse: 0.758 | rmse: 0.748 | LR: 0.002535 | Time: 0.6797351837158203\n",
      "train\n",
      "Epoch:  41 | loss: 0.00000 | rmse: 0.717 | rmse: 0.738 | rmse: 0.716 | LR: 0.002375 | Time: 0.6860747337341309\n",
      "train\n",
      "Epoch:  42 | loss: 0.00000 | rmse: 0.675 | rmse: 0.712 | rmse: 0.691 | LR: 0.002109 | Time: 0.6825299263000488\n",
      "train\n",
      "Epoch:  43 | loss: 0.00000 | rmse: 0.689 | rmse: 0.765 | rmse: 0.699 | LR: 0.001762 | Time: 0.6805429458618164\n",
      "train\n",
      "Epoch:  44 | loss: 0.00000 | rmse: 0.669 | rmse: 0.719 | rmse: 0.682 | LR: 0.001369 | Time: 0.6817121505737305\n",
      "train\n",
      "Epoch:  45 | loss: 0.00000 | rmse: 0.662 | rmse: 0.709 | rmse: 0.669 | LR: 0.000968 | Time: 0.6799461841583252\n",
      "train\n",
      "Epoch:  46 | loss: 0.00000 | rmse: 0.666 | rmse: 0.742 | rmse: 0.678 | LR: 0.000598 | Time: 0.6843891143798828\n",
      "train\n",
      "Epoch:  47 | loss: 0.00000 | rmse: 0.654 | rmse: 0.695 | rmse: 0.667 | LR: 0.000296 | Time: 0.6838679313659668\n",
      "train\n",
      "Epoch:  48 | loss: 0.00000 | rmse: 0.648 | rmse: 0.701 | rmse: 0.661 | LR: 0.000090 | Time: 0.6845109462738037\n",
      "train\n",
      "Epoch:  49 | loss: 0.00000 | rmse: 0.648 | rmse: 0.703 | rmse: 0.661 | LR: 0.000003 | Time: 0.6826870441436768\n",
      "train\n",
      "Epoch:  50 | loss: 0.00000 | rmse: 0.675 | rmse: 0.699 | rmse: 0.676 | LR: 0.002408 | Time: 0.6956110000610352\n",
      "train\n",
      "Epoch:  51 | loss: 0.00000 | rmse: 0.708 | rmse: 0.799 | rmse: 0.719 | LR: 0.002256 | Time: 0.6938259601593018\n",
      "train\n",
      "Epoch:  52 | loss: 0.00000 | rmse: 0.686 | rmse: 0.768 | rmse: 0.697 | LR: 0.002003 | Time: 0.6809980869293213\n",
      "train\n",
      "Epoch:  53 | loss: 0.00000 | rmse: 0.638 | rmse: 0.705 | rmse: 0.670 | LR: 0.001674 | Time: 0.6802270412445068\n",
      "train\n",
      "Epoch:  54 | loss: 0.00000 | rmse: 0.696 | rmse: 0.696 | rmse: 0.712 | LR: 0.001300 | Time: 0.6794271469116211\n",
      "train\n",
      "Epoch:  55 | loss: 0.00000 | rmse: 0.633 | rmse: 0.698 | rmse: 0.655 | LR: 0.000919 | Time: 0.694544792175293\n",
      "train\n",
      "Epoch:  56 | loss: 0.00000 | rmse: 0.631 | rmse: 0.696 | rmse: 0.649 | LR: 0.000568 | Time: 0.6990377902984619\n",
      "train\n",
      "Epoch:  57 | loss: 0.00000 | rmse: 0.624 | rmse: 0.686 | rmse: 0.645 | LR: 0.000281 | Time: 0.6948578357696533\n",
      "train\n",
      "Epoch:  58 | loss: 0.00000 | rmse: 0.628 | rmse: 0.674 | rmse: 0.649 | LR: 0.000086 | Time: 0.6935708522796631\n",
      "train\n",
      "Epoch:  59 | loss: 0.00000 | rmse: 0.625 | rmse: 0.675 | rmse: 0.646 | LR: 0.000002 | Time: 0.6969070434570312\n",
      "train\n",
      "Epoch:  60 | loss: 0.00000 | rmse: 0.949 | rmse: 1.056 | rmse: 0.970 | LR: 0.002288 | Time: 0.6905837059020996\n",
      "train\n",
      "Epoch:  61 | loss: 0.00000 | rmse: 0.837 | rmse: 0.948 | rmse: 0.842 | LR: 0.002144 | Time: 0.6861329078674316\n",
      "train\n",
      "Epoch:  62 | loss: 0.00000 | rmse: 0.830 | rmse: 0.924 | rmse: 0.850 | LR: 0.001903 | Time: 0.6792781352996826\n",
      "train\n",
      "Epoch:  63 | loss: 0.00000 | rmse: 0.773 | rmse: 0.880 | rmse: 0.776 | LR: 0.001590 | Time: 0.6831727027893066\n",
      "train\n",
      "Epoch:  64 | loss: 0.00000 | rmse: 0.672 | rmse: 0.688 | rmse: 0.686 | LR: 0.001235 | Time: 0.6818742752075195\n",
      "train\n",
      "Epoch:  65 | loss: 0.00000 | rmse: 0.636 | rmse: 0.669 | rmse: 0.649 | LR: 0.000873 | Time: 0.6810867786407471\n",
      "train\n",
      "Epoch:  66 | loss: 0.00000 | rmse: 0.626 | rmse: 0.688 | rmse: 0.644 | LR: 0.000540 | Time: 0.6915857791900635\n",
      "train\n",
      "Epoch:  67 | loss: 0.00000 | rmse: 0.621 | rmse: 0.678 | rmse: 0.641 | LR: 0.000267 | Time: 0.684791088104248\n",
      "train\n",
      "Epoch:  68 | loss: 0.00000 | rmse: 0.627 | rmse: 0.662 | rmse: 0.648 | LR: 0.000082 | Time: 0.6813449859619141\n",
      "train\n",
      "Epoch:  69 | loss: 0.00000 | rmse: 0.624 | rmse: 0.662 | rmse: 0.646 | LR: 0.000002 | Time: 0.6822998523712158\n",
      "train\n",
      "Epoch:  70 | loss: 0.00000 | rmse: 0.613 | rmse: 0.660 | rmse: 0.634 | LR: 0.002174 | Time: 0.6802520751953125\n",
      "train\n",
      "Epoch:  71 | loss: 0.00000 | rmse: 0.623 | rmse: 0.688 | rmse: 0.639 | LR: 0.002036 | Time: 0.6821470260620117\n",
      "train\n",
      "Epoch:  72 | loss: 0.00000 | rmse: 0.615 | rmse: 0.674 | rmse: 0.632 | LR: 0.001808 | Time: 0.682805061340332\n",
      "train\n",
      "Epoch:  73 | loss: 0.00000 | rmse: 0.608 | rmse: 0.657 | rmse: 0.638 | LR: 0.001511 | Time: 0.6945669651031494\n",
      "train\n",
      "Epoch:  74 | loss: 0.00000 | rmse: 0.610 | rmse: 0.639 | rmse: 0.632 | LR: 0.001173 | Time: 0.683408260345459\n",
      "train\n",
      "Epoch:  75 | loss: 0.00000 | rmse: 0.608 | rmse: 0.639 | rmse: 0.632 | LR: 0.000830 | Time: 0.6831977367401123\n",
      "train\n",
      "Epoch:  76 | loss: 0.00000 | rmse: 0.615 | rmse: 0.690 | rmse: 0.649 | LR: 0.000513 | Time: 0.6888000965118408\n",
      "train\n",
      "Epoch:  77 | loss: 0.00000 | rmse: 0.599 | rmse: 0.647 | rmse: 0.634 | LR: 0.000253 | Time: 0.6818559169769287\n",
      "train\n",
      "Epoch:  78 | loss: 0.00000 | rmse: 0.598 | rmse: 0.644 | rmse: 0.630 | LR: 0.000078 | Time: 0.6815700531005859\n",
      "train\n",
      "Epoch:  79 | loss: 0.00000 | rmse: 0.594 | rmse: 0.645 | rmse: 0.626 | LR: 0.000002 | Time: 0.6821942329406738\n",
      "train\n",
      "Epoch:  80 | loss: 0.00000 | rmse: 0.619 | rmse: 0.674 | rmse: 0.637 | LR: 0.002065 | Time: 0.6903281211853027\n",
      "train\n",
      "Epoch:  81 | loss: 0.00000 | rmse: 0.662 | rmse: 0.758 | rmse: 0.666 | LR: 0.001935 | Time: 0.6924648284912109\n",
      "train\n",
      "Epoch:  82 | loss: 0.00000 | rmse: 0.646 | rmse: 0.716 | rmse: 0.663 | LR: 0.001718 | Time: 0.6930248737335205\n",
      "train\n",
      "Epoch:  83 | loss: 0.00000 | rmse: 0.596 | rmse: 0.644 | rmse: 0.620 | LR: 0.001435 | Time: 0.6872949600219727\n",
      "train\n",
      "Epoch:  84 | loss: 0.00000 | rmse: 0.698 | rmse: 0.694 | rmse: 0.730 | LR: 0.001115 | Time: 0.6964011192321777\n",
      "train\n",
      "Epoch:  85 | loss: 0.00000 | rmse: 0.605 | rmse: 0.662 | rmse: 0.632 | LR: 0.000788 | Time: 0.7077829837799072\n",
      "train\n",
      "Epoch:  86 | loss: 0.00000 | rmse: 0.594 | rmse: 0.628 | rmse: 0.623 | LR: 0.000487 | Time: 0.7010817527770996\n",
      "train\n",
      "Epoch:  87 | loss: 0.00000 | rmse: 0.589 | rmse: 0.645 | rmse: 0.618 | LR: 0.000241 | Time: 0.6791839599609375\n",
      "train\n",
      "Epoch:  88 | loss: 0.00000 | rmse: 0.587 | rmse: 0.637 | rmse: 0.619 | LR: 0.000074 | Time: 0.6784420013427734\n",
      "train\n",
      "Epoch:  89 | loss: 0.00000 | rmse: 0.586 | rmse: 0.634 | rmse: 0.619 | LR: 0.000002 | Time: 0.696444034576416\n",
      "train\n",
      "Epoch:  90 | loss: 0.00000 | rmse: 0.586 | rmse: 0.621 | rmse: 0.627 | LR: 0.001962 | Time: 0.6943931579589844\n",
      "train\n",
      "Epoch:  91 | loss: 0.00000 | rmse: 0.580 | rmse: 0.633 | rmse: 0.620 | LR: 0.001838 | Time: 0.680387020111084\n",
      "train\n",
      "Epoch:  92 | loss: 0.00000 | rmse: 0.580 | rmse: 0.644 | rmse: 0.622 | LR: 0.001632 | Time: 0.681912899017334\n",
      "train\n",
      "Epoch:  93 | loss: 0.00000 | rmse: 0.576 | rmse: 0.637 | rmse: 0.623 | LR: 0.001363 | Time: 0.6811180114746094\n",
      "train\n",
      "Epoch:  94 | loss: 0.00000 | rmse: 0.587 | rmse: 0.617 | rmse: 0.632 | LR: 0.001059 | Time: 0.6803579330444336\n",
      "train\n",
      "Epoch:  95 | loss: 0.00000 | rmse: 0.568 | rmse: 0.629 | rmse: 0.616 | LR: 0.000749 | Time: 0.6786208152770996\n",
      "train\n",
      "Epoch:  96 | loss: 0.00000 | rmse: 0.566 | rmse: 0.620 | rmse: 0.615 | LR: 0.000463 | Time: 0.680229902267456\n",
      "train\n",
      "Epoch:  97 | loss: 0.00000 | rmse: 0.564 | rmse: 0.626 | rmse: 0.613 | LR: 0.000229 | Time: 0.6805210113525391\n",
      "train\n",
      "Epoch:  98 | loss: 0.00000 | rmse: 0.564 | rmse: 0.628 | rmse: 0.613 | LR: 0.000070 | Time: 0.6780221462249756\n",
      "train\n",
      "Epoch:  99 | loss: 0.00000 | rmse: 0.564 | rmse: 0.625 | rmse: 0.613 | LR: 0.000002 | Time: 0.6811749935150146\n",
      "train\n",
      "Epoch: 100 | loss: 0.00000 | rmse: 0.593 | rmse: 0.623 | rmse: 0.649 | LR: 0.001864 | Time: 0.6805989742279053\n",
      "train\n",
      "Epoch: 101 | loss: 0.00000 | rmse: 0.599 | rmse: 0.625 | rmse: 0.633 | LR: 0.001746 | Time: 0.6791057586669922\n",
      "train\n",
      "Epoch: 102 | loss: 0.00000 | rmse: 0.584 | rmse: 0.616 | rmse: 0.636 | LR: 0.001550 | Time: 0.6807980537414551\n",
      "train\n",
      "Epoch: 103 | loss: 0.00000 | rmse: 0.579 | rmse: 0.608 | rmse: 0.642 | LR: 0.001295 | Time: 0.6806280612945557\n",
      "train\n",
      "Epoch: 104 | loss: 0.00000 | rmse: 0.621 | rmse: 0.630 | rmse: 0.661 | LR: 0.001006 | Time: 0.7011349201202393\n",
      "train\n",
      "Epoch: 105 | loss: 0.00000 | rmse: 0.578 | rmse: 0.607 | rmse: 0.622 | LR: 0.000711 | Time: 0.6875178813934326\n",
      "train\n",
      "Epoch: 106 | loss: 0.00000 | rmse: 0.573 | rmse: 0.647 | rmse: 0.620 | LR: 0.000439 | Time: 0.6871342658996582\n",
      "train\n",
      "Epoch: 107 | loss: 0.00000 | rmse: 0.559 | rmse: 0.609 | rmse: 0.609 | LR: 0.000217 | Time: 0.6803600788116455\n",
      "train\n",
      "Epoch: 108 | loss: 0.00000 | rmse: 0.557 | rmse: 0.610 | rmse: 0.604 | LR: 0.000066 | Time: 0.6813209056854248\n",
      "train\n",
      "Epoch: 109 | loss: 0.00000 | rmse: 0.557 | rmse: 0.612 | rmse: 0.604 | LR: 0.000002 | Time: 0.6790149211883545\n",
      "train\n",
      "Epoch: 110 | loss: 0.00000 | rmse: 0.618 | rmse: 0.707 | rmse: 0.664 | LR: 0.001770 | Time: 0.6816470623016357\n",
      "train\n",
      "Epoch: 111 | loss: 0.00000 | rmse: 0.559 | rmse: 0.602 | rmse: 0.612 | LR: 0.001659 | Time: 0.6823313236236572\n",
      "train\n",
      "Epoch: 112 | loss: 0.00000 | rmse: 0.589 | rmse: 0.607 | rmse: 0.633 | LR: 0.001473 | Time: 0.6883518695831299\n",
      "train\n",
      "Epoch: 113 | loss: 0.00000 | rmse: 0.554 | rmse: 0.602 | rmse: 0.613 | LR: 0.001230 | Time: 0.6964030265808105\n",
      "train\n",
      "Epoch: 114 | loss: 0.00000 | rmse: 0.551 | rmse: 0.609 | rmse: 0.607 | LR: 0.000956 | Time: 0.6977312564849854\n",
      "train\n",
      "Epoch: 115 | loss: 0.00000 | rmse: 0.545 | rmse: 0.609 | rmse: 0.603 | LR: 0.000676 | Time: 0.7019939422607422\n",
      "train\n",
      "Epoch: 116 | loss: 0.00000 | rmse: 0.544 | rmse: 0.616 | rmse: 0.604 | LR: 0.000417 | Time: 0.7004959583282471\n",
      "train\n",
      "Epoch: 117 | loss: 0.00000 | rmse: 0.544 | rmse: 0.606 | rmse: 0.604 | LR: 0.000206 | Time: 0.6900060176849365\n",
      "train\n",
      "Epoch: 118 | loss: 0.00000 | rmse: 0.545 | rmse: 0.603 | rmse: 0.607 | LR: 0.000063 | Time: 0.6994001865386963\n",
      "train\n",
      "Epoch: 119 | loss: 0.00000 | rmse: 0.542 | rmse: 0.605 | rmse: 0.604 | LR: 0.000002 | Time: 0.7007479667663574\n",
      "train\n",
      "Epoch: 120 | loss: 0.00000 | rmse: 0.672 | rmse: 0.773 | rmse: 0.693 | LR: 0.001682 | Time: 0.7033989429473877\n",
      "train\n",
      "Epoch: 121 | loss: 0.00000 | rmse: 0.564 | rmse: 0.629 | rmse: 0.599 | LR: 0.001576 | Time: 0.6962442398071289\n",
      "train\n",
      "Epoch: 122 | loss: 0.00000 | rmse: 0.547 | rmse: 0.595 | rmse: 0.608 | LR: 0.001399 | Time: 0.6922869682312012\n",
      "train\n",
      "Epoch: 123 | loss: 0.00000 | rmse: 0.549 | rmse: 0.596 | rmse: 0.616 | LR: 0.001169 | Time: 0.6889209747314453\n",
      "train\n",
      "Epoch: 124 | loss: 0.00000 | rmse: 0.546 | rmse: 0.595 | rmse: 0.605 | LR: 0.000908 | Time: 0.6923463344573975\n",
      "train\n",
      "Epoch: 125 | loss: 0.00000 | rmse: 0.541 | rmse: 0.617 | rmse: 0.605 | LR: 0.000642 | Time: 0.695673942565918\n",
      "train\n",
      "Epoch: 126 | loss: 0.00000 | rmse: 0.536 | rmse: 0.603 | rmse: 0.607 | LR: 0.000397 | Time: 0.6922481060028076\n",
      "train\n",
      "Epoch: 127 | loss: 0.00000 | rmse: 0.534 | rmse: 0.595 | rmse: 0.599 | LR: 0.000196 | Time: 0.6898729801177979\n",
      "train\n",
      "Epoch: 128 | loss: 0.00000 | rmse: 0.535 | rmse: 0.592 | rmse: 0.600 | LR: 0.000060 | Time: 0.6911752223968506\n",
      "train\n",
      "Epoch: 129 | loss: 0.00000 | rmse: 0.535 | rmse: 0.593 | rmse: 0.600 | LR: 0.000002 | Time: 0.683610200881958\n",
      "train\n",
      "Epoch: 130 | loss: 0.00000 | rmse: 0.550 | rmse: 0.589 | rmse: 0.608 | LR: 0.001598 | Time: 0.6827337741851807\n",
      "train\n",
      "Epoch: 131 | loss: 0.00000 | rmse: 0.532 | rmse: 0.598 | rmse: 0.600 | LR: 0.001497 | Time: 0.6805670261383057\n",
      "train\n",
      "Epoch: 132 | loss: 0.00000 | rmse: 0.612 | rmse: 0.639 | rmse: 0.691 | LR: 0.001329 | Time: 0.6811971664428711\n",
      "train\n",
      "Epoch: 133 | loss: 0.00000 | rmse: 0.534 | rmse: 0.608 | rmse: 0.587 | LR: 0.001110 | Time: 0.6844532489776611\n",
      "train\n",
      "Epoch: 134 | loss: 0.00000 | rmse: 0.528 | rmse: 0.600 | rmse: 0.601 | LR: 0.000863 | Time: 0.6826789379119873\n",
      "train\n",
      "Epoch: 135 | loss: 0.00000 | rmse: 0.527 | rmse: 0.609 | rmse: 0.593 | LR: 0.000610 | Time: 0.6818530559539795\n",
      "train\n",
      "Epoch: 136 | loss: 0.00000 | rmse: 0.525 | rmse: 0.603 | rmse: 0.590 | LR: 0.000377 | Time: 0.6871590614318848\n",
      "train\n",
      "Epoch: 137 | loss: 0.00000 | rmse: 0.524 | rmse: 0.598 | rmse: 0.591 | LR: 0.000186 | Time: 0.6822211742401123\n",
      "train\n",
      "Epoch: 138 | loss: 0.00000 | rmse: 0.523 | rmse: 0.595 | rmse: 0.592 | LR: 0.000057 | Time: 0.6852421760559082\n",
      "train\n",
      "Epoch: 139 | loss: 0.00000 | rmse: 0.522 | rmse: 0.598 | rmse: 0.591 | LR: 0.000002 | Time: 0.6854357719421387\n",
      "train\n",
      "Epoch: 140 | loss: 0.00000 | rmse: 0.533 | rmse: 0.602 | rmse: 0.588 | LR: 0.001518 | Time: 0.6834440231323242\n",
      "train\n",
      "Epoch: 141 | loss: 0.00000 | rmse: 0.525 | rmse: 0.610 | rmse: 0.598 | LR: 0.001422 | Time: 0.685211181640625\n",
      "train\n",
      "Epoch: 142 | loss: 0.00000 | rmse: 0.523 | rmse: 0.595 | rmse: 0.589 | LR: 0.001263 | Time: 0.6847970485687256\n",
      "train\n",
      "Epoch: 143 | loss: 0.00000 | rmse: 0.558 | rmse: 0.600 | rmse: 0.636 | LR: 0.001055 | Time: 0.6878352165222168\n",
      "train\n",
      "Epoch: 144 | loss: 0.00000 | rmse: 0.558 | rmse: 0.601 | rmse: 0.634 | LR: 0.000819 | Time: 0.6816368103027344\n",
      "train\n",
      "Epoch: 145 | loss: 0.00000 | rmse: 0.534 | rmse: 0.586 | rmse: 0.608 | LR: 0.000579 | Time: 0.6853210926055908\n",
      "train\n",
      "Epoch: 146 | loss: 0.00000 | rmse: 0.562 | rmse: 0.663 | rmse: 0.619 | LR: 0.000358 | Time: 0.6829791069030762\n",
      "train\n",
      "Epoch: 147 | loss: 0.00000 | rmse: 0.532 | rmse: 0.586 | rmse: 0.606 | LR: 0.000177 | Time: 0.6873979568481445\n",
      "train\n",
      "Epoch: 148 | loss: 0.00000 | rmse: 0.518 | rmse: 0.604 | rmse: 0.590 | LR: 0.000054 | Time: 0.6872701644897461\n",
      "train\n",
      "Epoch: 149 | loss: 0.00000 | rmse: 0.517 | rmse: 0.602 | rmse: 0.590 | LR: 0.000002 | Time: 0.6834549903869629\n",
      "train\n",
      "Epoch: 150 | loss: 0.00000 | rmse: 0.528 | rmse: 0.587 | rmse: 0.591 | LR: 0.001442 | Time: 0.685917854309082\n",
      "train\n",
      "Epoch: 151 | loss: 0.00000 | rmse: 0.568 | rmse: 0.668 | rmse: 0.621 | LR: 0.001351 | Time: 0.6826162338256836\n",
      "train\n",
      "Epoch: 152 | loss: 0.00000 | rmse: 0.518 | rmse: 0.581 | rmse: 0.598 | LR: 0.001199 | Time: 0.6851279735565186\n",
      "train\n",
      "Epoch: 153 | loss: 0.00000 | rmse: 0.528 | rmse: 0.592 | rmse: 0.600 | LR: 0.001002 | Time: 0.6885900497436523\n",
      "train\n",
      "Epoch: 154 | loss: 0.00000 | rmse: 0.518 | rmse: 0.585 | rmse: 0.592 | LR: 0.000779 | Time: 0.685502290725708\n",
      "train\n",
      "Epoch: 155 | loss: 0.00000 | rmse: 0.517 | rmse: 0.604 | rmse: 0.599 | LR: 0.000550 | Time: 0.6843268871307373\n",
      "train\n",
      "Epoch: 156 | loss: 0.00000 | rmse: 0.512 | rmse: 0.588 | rmse: 0.593 | LR: 0.000340 | Time: 0.6829917430877686\n",
      "train\n",
      "Epoch: 157 | loss: 0.00000 | rmse: 0.509 | rmse: 0.591 | rmse: 0.590 | LR: 0.000168 | Time: 0.6862258911132812\n",
      "train\n",
      "Epoch: 158 | loss: 0.00000 | rmse: 0.509 | rmse: 0.588 | rmse: 0.592 | LR: 0.000051 | Time: 0.6847250461578369\n",
      "train\n",
      "Epoch: 159 | loss: 0.00000 | rmse: 0.509 | rmse: 0.587 | rmse: 0.592 | LR: 0.000001 | Time: 0.6835012435913086\n",
      "train\n",
      "Epoch: 160 | loss: 0.00000 | rmse: 0.513 | rmse: 0.589 | rmse: 0.577 | LR: 0.001370 | Time: 0.6849889755249023\n",
      "train\n",
      "Epoch: 161 | loss: 0.00000 | rmse: 0.512 | rmse: 0.582 | rmse: 0.588 | LR: 0.001283 | Time: 0.6893589496612549\n",
      "train\n",
      "Epoch: 162 | loss: 0.00000 | rmse: 0.563 | rmse: 0.606 | rmse: 0.650 | LR: 0.001139 | Time: 0.683967113494873\n",
      "train\n",
      "Epoch: 163 | loss: 0.00000 | rmse: 0.526 | rmse: 0.582 | rmse: 0.590 | LR: 0.000952 | Time: 0.6846849918365479\n",
      "train\n",
      "Epoch: 164 | loss: 0.00000 | rmse: 0.517 | rmse: 0.591 | rmse: 0.609 | LR: 0.000740 | Time: 0.6826229095458984\n",
      "train\n",
      "Epoch: 165 | loss: 0.00000 | rmse: 0.505 | rmse: 0.588 | rmse: 0.594 | LR: 0.000523 | Time: 0.690925121307373\n",
      "train\n",
      "Epoch: 166 | loss: 0.00000 | rmse: 0.516 | rmse: 0.617 | rmse: 0.588 | LR: 0.000323 | Time: 0.6842429637908936\n",
      "train\n",
      "Epoch: 167 | loss: 0.00000 | rmse: 0.503 | rmse: 0.584 | rmse: 0.588 | LR: 0.000160 | Time: 0.6865861415863037\n",
      "train\n",
      "Epoch: 168 | loss: 0.00000 | rmse: 0.502 | rmse: 0.590 | rmse: 0.587 | LR: 0.000049 | Time: 0.6842570304870605\n",
      "train\n",
      "Epoch: 169 | loss: 0.00000 | rmse: 0.502 | rmse: 0.590 | rmse: 0.586 | LR: 0.000001 | Time: 0.6819987297058105\n",
      "train\n",
      "Epoch: 170 | loss: 0.00000 | rmse: 0.538 | rmse: 0.597 | rmse: 0.628 | LR: 0.001301 | Time: 0.6857459545135498\n",
      "train\n",
      "Epoch: 171 | loss: 0.00000 | rmse: 0.661 | rmse: 0.772 | rmse: 0.695 | LR: 0.001219 | Time: 0.6882660388946533\n",
      "train\n",
      "Epoch: 172 | loss: 0.00000 | rmse: 0.555 | rmse: 0.651 | rmse: 0.619 | LR: 0.001083 | Time: 0.6850571632385254\n",
      "train\n",
      "Epoch: 173 | loss: 0.00000 | rmse: 0.576 | rmse: 0.684 | rmse: 0.628 | LR: 0.000904 | Time: 0.6821627616882324\n",
      "train\n",
      "Epoch: 174 | loss: 0.00000 | rmse: 0.610 | rmse: 0.724 | rmse: 0.671 | LR: 0.000703 | Time: 0.6889903545379639\n",
      "train\n",
      "Epoch: 175 | loss: 0.00000 | rmse: 0.503 | rmse: 0.584 | rmse: 0.581 | LR: 0.000497 | Time: 0.694756031036377\n",
      "train\n",
      "Epoch: 176 | loss: 0.00000 | rmse: 0.519 | rmse: 0.580 | rmse: 0.603 | LR: 0.000307 | Time: 0.6839067935943604\n",
      "train\n",
      "Epoch: 177 | loss: 0.00000 | rmse: 0.502 | rmse: 0.586 | rmse: 0.587 | LR: 0.000152 | Time: 0.6860611438751221\n",
      "train\n",
      "Epoch: 178 | loss: 0.00000 | rmse: 0.506 | rmse: 0.596 | rmse: 0.589 | LR: 0.000046 | Time: 0.6863949298858643\n",
      "train\n",
      "Epoch: 179 | loss: 0.00000 | rmse: 0.502 | rmse: 0.589 | rmse: 0.587 | LR: 0.000001 | Time: 0.6814699172973633\n",
      "train\n",
      "Epoch: 180 | loss: 0.00000 | rmse: 0.711 | rmse: 0.741 | rmse: 0.772 | LR: 0.001236 | Time: 0.6844019889831543\n",
      "train\n",
      "Epoch: 181 | loss: 0.00000 | rmse: 0.691 | rmse: 0.708 | rmse: 0.725 | LR: 0.001158 | Time: 0.6834790706634521\n",
      "train\n",
      "Epoch: 182 | loss: 0.00000 | rmse: 0.571 | rmse: 0.608 | rmse: 0.646 | LR: 0.001028 | Time: 0.6843991279602051\n",
      "train\n",
      "Epoch: 183 | loss: 0.00000 | rmse: 0.527 | rmse: 0.579 | rmse: 0.603 | LR: 0.000859 | Time: 0.6852622032165527\n",
      "train\n",
      "Epoch: 184 | loss: 0.00000 | rmse: 0.525 | rmse: 0.618 | rmse: 0.594 | LR: 0.000667 | Time: 0.6884880065917969\n",
      "train\n",
      "Epoch: 185 | loss: 0.00000 | rmse: 0.520 | rmse: 0.618 | rmse: 0.595 | LR: 0.000472 | Time: 0.6895442008972168\n",
      "train\n",
      "Epoch: 186 | loss: 0.00000 | rmse: 0.515 | rmse: 0.580 | rmse: 0.598 | LR: 0.000292 | Time: 0.6823768615722656\n",
      "train\n",
      "Epoch: 187 | loss: 0.00000 | rmse: 0.502 | rmse: 0.586 | rmse: 0.580 | LR: 0.000144 | Time: 0.6820988655090332\n",
      "train\n",
      "Epoch: 188 | loss: 0.00000 | rmse: 0.505 | rmse: 0.593 | rmse: 0.583 | LR: 0.000044 | Time: 0.6820158958435059\n",
      "train\n",
      "Epoch: 189 | loss: 0.00000 | rmse: 0.502 | rmse: 0.588 | rmse: 0.582 | LR: 0.000001 | Time: 0.6818430423736572\n",
      "train\n",
      "Epoch: 190 | loss: 0.00000 | rmse: 0.521 | rmse: 0.582 | rmse: 0.597 | LR: 0.001175 | Time: 0.6849048137664795\n",
      "train\n",
      "Epoch: 191 | loss: 0.00000 | rmse: 0.566 | rmse: 0.609 | rmse: 0.652 | LR: 0.001100 | Time: 0.6852447986602783\n",
      "train\n",
      "Epoch: 192 | loss: 0.00000 | rmse: 0.544 | rmse: 0.592 | rmse: 0.627 | LR: 0.000977 | Time: 0.6884100437164307\n",
      "train\n",
      "Epoch: 193 | loss: 0.00000 | rmse: 0.532 | rmse: 0.588 | rmse: 0.625 | LR: 0.000816 | Time: 0.6851470470428467\n",
      "train\n",
      "Epoch: 194 | loss: 0.00000 | rmse: 0.498 | rmse: 0.581 | rmse: 0.591 | LR: 0.000634 | Time: 0.682528018951416\n",
      "train\n",
      "Epoch: 195 | loss: 0.00000 | rmse: 0.496 | rmse: 0.580 | rmse: 0.580 | LR: 0.000448 | Time: 0.6806111335754395\n",
      "train\n",
      "Epoch: 196 | loss: 0.00000 | rmse: 0.497 | rmse: 0.596 | rmse: 0.590 | LR: 0.000277 | Time: 0.6866471767425537\n",
      "train\n",
      "Epoch: 197 | loss: 0.00000 | rmse: 0.494 | rmse: 0.585 | rmse: 0.588 | LR: 0.000137 | Time: 0.6845457553863525\n",
      "train\n",
      "Epoch: 198 | loss: 0.00000 | rmse: 0.494 | rmse: 0.581 | rmse: 0.587 | LR: 0.000042 | Time: 0.6811492443084717\n",
      "train\n",
      "Epoch: 199 | loss: 0.00000 | rmse: 0.494 | rmse: 0.582 | rmse: 0.586 | LR: 0.000001 | Time: 0.6923770904541016\n",
      "\n",
      "==> Best test mse: 0.333,  rmse: 0.577\n"
     ]
    }
   ],
   "source": [
    "device = mx.gpu # or mx.cpu\n",
    "mx.set_default_device(device)\n",
    "\n",
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([canonical_smiles_list[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "x_atom = mx.array(x_atom)\n",
    "x_bonds = mx.array(x_bonds)\n",
    "x_atom_index = mx.array(x_atom_index)\n",
    "x_bond_index = mx.array(x_bond_index)\n",
    "x_mask = mx.array(x_mask)\n",
    "\n",
    "model = AttFP(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "\n",
    "# Example: Dynamically create learning rate schedules based on milestones\n",
    "initial_lr = 10**-learning_rate\n",
    "restarts = 20\n",
    "decay_step = 10*5  # Decay steps for each cosine and warmup phase\n",
    "warmup_factor = 0.95  # Warmup reduction factors\n",
    "\n",
    "lr_schedule = cosineannealingwarmrestartfactor(initial_lr, restarts, decay_step, warmup_factor)\n",
    "\n",
    "optimizer = optim.AdamW(learning_rate=lr_schedule, weight_decay=1**-weight_decay)\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def loss_fn(y_hat, y):\n",
    "    y = mx.reshape(y, y_hat.shape)\n",
    "    return mx.mean(nn.losses.mse_loss(y_hat, y))\n",
    "\n",
    "\n",
    "state = [model, optimizer.state, mx.random.state]\n",
    "\n",
    "def forward_fn(x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, labels):\n",
    "    _, y_hat = model(x_atom, x_bonds, x_atom_index, x_bond_index, x_mask)\n",
    "    loss = loss_fn(y_hat, labels)\n",
    "    return loss, y_hat\n",
    "\n",
    "@partial(mx.compile, inputs=state, outputs=state)\n",
    "def step(x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, labels):\n",
    "    loss_and_grad_fn = nn.value_and_grad(model, forward_fn)\n",
    "    (loss, y_hat), grads = loss_and_grad_fn(\n",
    "        x_atom=x_atom,\n",
    "        x_bonds=x_bonds,\n",
    "        x_atom_index=x_atom_index,\n",
    "        x_bond_index=x_bond_index,\n",
    "        x_mask=x_mask,\n",
    "        labels=labels,\n",
    "    )\n",
    "    optimizer.update(model, grads)\n",
    "    return loss\n",
    "        \n",
    "def train(dataset,e, batch_size=64, doprofile=False):\n",
    "    print('train')\n",
    "    loss_sum = 0.0\n",
    "    np.random.seed(e)\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)   \n",
    "    #print('iter per batch:',len(batch_list))\n",
    "    if doprofile:\n",
    "        pr = cProfile.Profile()\n",
    "        pr.enable()  # Start profiling\n",
    "\n",
    "    for counter, train_batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[train_batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        y_val = mx.array(batch_df[tasks[0]].values)\n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom = mx.array(x_atom)\n",
    "        x_bonds = mx.array(x_bonds)\n",
    "        x_atom_index = mx.array(x_atom_index)\n",
    "        x_bond_index = mx.array(x_bond_index)\n",
    "        x_mask = mx.array(x_mask)\n",
    "\n",
    "        loss = step(x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, y_val)\n",
    "        mx.eval(state)\n",
    "        \n",
    "    if doprofile:\n",
    "        pr.disable()\n",
    "        # Print profiling results\n",
    "        s = io.StringIO()\n",
    "        ps = pstats.Stats(pr, stream=s).sort_stats(pstats.SortKey.CUMULATIVE)\n",
    "        ps.print_stats(200)  # Print top 10 results\n",
    "        print(s.getvalue())    \n",
    "    return loss_sum / len(dataset)\n",
    "\n",
    "def test(test_dataset, batch_size=64):\n",
    "    mse= 0.0\n",
    "    valList = np.arange(0,test_dataset.shape[0])\n",
    "    batch_list = []\n",
    "    for i in range(0, test_dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    for counter, test_batch in enumerate(batch_list):\n",
    "        batch_df = test_dataset.loc[test_batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, _ = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom = mx.array(x_atom)\n",
    "        x_bonds = mx.array(x_bonds)\n",
    "        x_atom_index = mx.array(x_atom_index)\n",
    "        x_bond_index = mx.array(x_bond_index)\n",
    "        x_mask = mx.array(x_mask)\n",
    "        y_val = mx.array(batch_df[tasks[0]].values)\n",
    "        _, y_hat = model(x_atom, x_bonds, x_atom_index, x_bond_index, x_mask)\n",
    "        y_val = mx.reshape(y_val, y_hat.shape)\n",
    "        mse += mx.square(y_hat - y_val).sum().item()\n",
    "        \n",
    "    val =  mse / len(test_dataset)\n",
    "    model.train()\n",
    "    return val, np.sqrt(val)\n",
    "\n",
    "\n",
    "def epoch(e, batch_size=64):\n",
    "    loss = train(train_df,e, batch_size=batch_size)\n",
    "    train_mse, train_rmse = test(train_df, batch_size=2*batch_size)\n",
    "    valid_mse, valid_rmse = test(valid_df, batch_size=2*batch_size)\n",
    "\n",
    "    test_mse, test_rmse = test(test_df, batch_size=batch_size)\n",
    "    return loss, train_mse, train_rmse, valid_mse, valid_rmse,  test_mse, test_rmse\n",
    "\n",
    "\n",
    "r = []\n",
    "best_test_mse = 1e9\n",
    "print(batch_size)\n",
    "for e in range(epochs):\n",
    "    starttime = time.time()\n",
    "    \n",
    "    loss, train_mse, train_rmse, valid_mse, valid_rmse, test_mse, test_rmse = epoch(e, batch_size=batch_size)\n",
    "    stoptime = time.time()\n",
    "\n",
    "    #print('RAM memory % used:', psutil.virtual_memory()[2],'RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)\n",
    "\n",
    "    best_test_mse = min(best_test_mse, test_mse)\n",
    "    r.append((train_rmse,test_rmse))\n",
    "    print(\n",
    "        \" | \".join(\n",
    "            [\n",
    "                f\"Epoch: {e:3d}\",\n",
    "                f\"loss: {loss:.5f}\",\n",
    "                f\"rmse: {train_rmse:.3f}\",\n",
    "                f\"rmse: {valid_rmse:.3f}\",\n",
    "                f\"rmse: {test_rmse:.3f}\",\n",
    "                f\"LR: {np.array(optimizer.learning_rate):.6f}\",\n",
    "                f\"Time: {stoptime-starttime}\",\n",
    "\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "print(f\"\\n==> Best test mse: {best_test_mse:.3f},  rmse: {np.sqrt(best_test_mse):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e206fe80-78df-4b47-a8d7-e7a52653af8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
