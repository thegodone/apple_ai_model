{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2823024-9cc5-4462-b4d5-96a6d11506c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from rdkit import Chem\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import time\n",
    "from featurer import  save_smiles_dicts, get_smiles_dicts, get_smiles_array\n",
    "from AttFP_tf_utils import Fingerprint, CosineAnnealingLR_with_Restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c4954c-cfb6-47a4-b53b-8f9761c08690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 15:36:08.519830: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Max\n",
      "2024-10-27 15:36:08.519858: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 128.00 GB\n",
      "2024-10-27 15:36:08.519863: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 48.00 GB\n",
      "2024-10-27 15:36:08.519891: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-10-27 15:36:08.519908: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atom_feature shape: (200, 56, 192)\n",
      "atom_neighbor shape: (200, 56, 6, 39)\n",
      "bond_neighbor shape: (200, 56, 6, 10)\n",
      "Concatenated neighbor_feature shape: (200, 56, 6, 49)\n",
      "Processed neighbor_feature shape: (200, 56, 6, 192)\n",
      "feature_align shape: (200, 56, 6, 384)\n",
      "align_score shape: (200, 56, 6, 1)\n",
      "attention_weight shape: (200, 56, 6, 1)\n",
      "context shape after reduce_sum: (200, 56, 192)\n",
      "atom_features_reshape shape after GRU: (11200, 192)\n",
      "neighbor_feature shape at radius 0: (200, 56, 6, 192)\n",
      "feature_align shape at radius 0: (200, 56, 6, 384)\n",
      "align_score shape at radius 0: (200, 56, 6, 1)\n",
      "attention_weight shape at radius 0: (200, 56, 6, 1)\n",
      "context shape after reduce_sum at radius 0: (200, 56, 192)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([200, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example instantiation and testing\n",
    "model = Fingerprint(2, 2, 39, 10, 192, 1,0.05)\n",
    "x_atom = tf.random.normal((200, 56, 39))  # atom features\n",
    "x_bonds = tf.random.normal((200, 63, 10))  # bond features\n",
    "x_atom_index = tf.random.uniform((200, 56, 6), minval=0, maxval=56, dtype=tf.int32)  # atom neighbor indices\n",
    "x_bond_index = tf.random.uniform((200, 56, 6), minval=0, maxval=63, dtype=tf.int32)  # bond neighbor indices\n",
    "x_mask = tf.ones((200, 56))  # atom mask\n",
    "\n",
    "target = model([x_atom, x_bonds, x_atom_index, x_bond_index, x_mask], doprint=True)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae66f4f7-0fd8-4c0a-8c0b-6cdc3f4b2bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 ms ± 266 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model([x_atom, x_bonds, x_atom_index, x_bond_index, x_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40764b56-075f-4c44-8d28-49f1adb1e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 108 \n",
    "start_time = str(time.ctime()).replace(':','-').replace(' ','-')\n",
    "batch_size = 200\n",
    "epochs = 200\n",
    "p_dropout= 0.05\n",
    "fingerprint_dim = 192\n",
    "\n",
    "weight_decay = 5 # also known as l2_regularization_lambda\n",
    "learning_rate = 2.5\n",
    "output_units_num = 1 # for regression model\n",
    "radius = 2\n",
    "T = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16308f96-d41e-4498-aee7-5e6bff1910fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  1128\n",
      "number of successfully processed smiles:  1128\n"
     ]
    }
   ],
   "source": [
    "\n",
    "task_name = 'solubility'\n",
    "tasks = ['measured log solubility in mols per litre']\n",
    "\n",
    "raw_filename = \"delaney-processed.csv\"\n",
    "feature_filename = raw_filename.replace('.csv','.pickle')\n",
    "filename = raw_filename.replace('.csv','')\n",
    "prefix_filename = raw_filename.split('/')[-1].replace('.csv','')\n",
    "smiles_tasks_df = pd.read_csv(raw_filename)\n",
    "smilesList = smiles_tasks_df.smiles.values\n",
    "print(\"number of all smiles: \",len(smilesList))\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        remained_smiles.append(smiles)\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "    except:\n",
    "        print(smiles)\n",
    "        pass\n",
    "print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "smiles_tasks_df = smiles_tasks_df[smiles_tasks_df[\"smiles\"].isin(remained_smiles)]\n",
    "# print(smiles_tasks_df)\n",
    "smiles_tasks_df['cano_smiles'] =canonical_smiles_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2320c112-d453-4a19-8173-c4ffad08520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not processed items\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Compound ID</th>\n",
       "      <th>ESOL predicted log solubility in mols per litre</th>\n",
       "      <th>Minimum Degree</th>\n",
       "      <th>Molecular Weight</th>\n",
       "      <th>Number of H-Bond Donors</th>\n",
       "      <th>Number of Rings</th>\n",
       "      <th>Number of Rotatable Bonds</th>\n",
       "      <th>Polar Surface Area</th>\n",
       "      <th>measured log solubility in mols per litre</th>\n",
       "      <th>smiles</th>\n",
       "      <th>cano_smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>Methane</td>\n",
       "      <td>-0.636</td>\n",
       "      <td>0</td>\n",
       "      <td>16.043</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Compound ID  ESOL predicted log solubility in mols per litre  \\\n",
       "934     Methane                                           -0.636   \n",
       "\n",
       "     Minimum Degree  Molecular Weight  Number of H-Bond Donors  \\\n",
       "934               0            16.043                        0   \n",
       "\n",
       "     Number of Rings  Number of Rotatable Bonds  Polar Surface Area  \\\n",
       "934                0                          0                 0.0   \n",
       "\n",
       "     measured log solubility in mols per litre smiles cano_smiles  \n",
       "934                                       -0.9      C           C  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if os.path.isfile(feature_filename):\n",
    "    feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "else:\n",
    "    feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "# feature_dicts = get_smiles_dicts(smilesList)\n",
    "remained_df = smiles_tasks_df[smiles_tasks_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = smiles_tasks_df.drop(remained_df.index)\n",
    "print(\"not processed items\")\n",
    "uncovered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d5108da-f0e8-447a-9674-25c13340be27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fingerprint_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " atom_fc (Dense)             multiple                  7680      \n",
      "                                                                 \n",
      " neighbor_fc (Dense)         multiple                  9600      \n",
      "                                                                 \n",
      " atomgru_0 (GRUCell)         multiple                  222336    \n",
      "                                                                 \n",
      " atomgru_1 (GRUCell)         multiple                  222336    \n",
      "                                                                 \n",
      " align_0 (Dense)             multiple                  385       \n",
      "                                                                 \n",
      " align_1 (Dense)             multiple                  385       \n",
      "                                                                 \n",
      " attend_0 (Dense)            multiple                  37056     \n",
      "                                                                 \n",
      " attend_1 (Dense)            multiple                  37056     \n",
      "                                                                 \n",
      " molgru (GRUCell)            multiple                  222336    \n",
      "                                                                 \n",
      " molalign (Dense)            multiple                  385       \n",
      "                                                                 \n",
      " molattend (Dense)           multiple                  37056     \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " linear1 (Dense)             multiple                  37056     \n",
      "                                                                 \n",
      " linear2 (Dense)             multiple                  37056     \n",
      "                                                                 \n",
      " scalar (Dense)              multiple                  2         \n",
      "                                                                 \n",
      " output (Dense)              multiple                  193       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 870918 (3.32 MB)\n",
      "Trainable params: 870918 (3.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "remained_df = remained_df.reset_index(drop=True)\n",
    "test_df = remained_df.sample(frac=1/10, random_state=random_seed) # test set\n",
    "training_data = remained_df.drop(test_df.index) # training data\n",
    "\n",
    "# training data is further divided into validation set and train set\n",
    "valid_df = training_data.sample(frac=1/9, random_state=random_seed) # validation set\n",
    "train_df = training_data.drop(valid_df.index) # train set\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# Assuming canonical_smiles_list and feature_dicts are available\n",
    "x_atom_train, x_bonds_train, x_atom_index_train, x_bond_index_train, x_mask_train, smiles_to_rdkit_list_train = get_smiles_array(train_df['cano_smiles'], feature_dicts)\n",
    "x_atom_val, x_bonds_val, x_atom_index_val, x_bond_index_val, x_mask_val, smiles_to_rdkit_list_val = get_smiles_array(valid_df['cano_smiles'], feature_dicts)\n",
    "x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(test_df['cano_smiles'], feature_dicts)\n",
    "\n",
    "x_atom_index_train = tf.cast(x_atom_index_train, tf.int32)\n",
    "x_atom_index_val = tf.cast(x_atom_index_val, tf.int32)\n",
    "x_atom_index_test = tf.cast(x_atom_index_test, tf.int32)\n",
    "\n",
    "x_bond_index_train = tf.cast(x_bond_index_train, tf.int32)\n",
    "x_bond_index_val = tf.cast(x_bond_index_val, tf.int32)\n",
    "x_bond_index_test = tf.cast(x_bond_index_test, tf.int32)\n",
    "\n",
    "\n",
    "# Convert targets into NumPy arrays\n",
    "y_train = train_df[tasks[0]].values\n",
    "y_val = valid_df[tasks[0]].values\n",
    "y_test = test_df[tasks[0]].values\n",
    "\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_val = y_val.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "num_atom_features = x_atom_train.shape[-1]\n",
    "num_bond_features = x_bonds_train.shape[-1]\n",
    "print(num_atom_features,num_bond_features)\n",
    "\n",
    "# Move your model to MPS\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features, fingerprint_dim, output_units_num, p_dropout)\n",
    "\n",
    "mol_prediction = model([x_atom_train, x_bonds_train, x_atom_index_train, x_bond_index_train, x_mask_train])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=10**-2.5), \n",
    "              loss='mse', \n",
    "              metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8da3b8db-f5b2-4365-99b0-ed4aa562b4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.AdamW` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.AdamW`.\n"
     ]
    }
   ],
   "source": [
    "# Prepare an optimizer\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "\n",
    "# Define a loss function (mean squared error in this case)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")  # Track training loss\n",
    "val_loss = tf.keras.metrics.Mean(name=\"val_loss\")      # Track validation loss\n",
    "train_mse = tf.keras.metrics.MeanSquaredError(name=\"train_mse\")  # Track RMSE\n",
    "val_mse = tf.keras.metrics.MeanSquaredError(name=\"val_mse\")      # Track RMSE\n",
    "test_mse = tf.keras.metrics.MeanSquaredError(name=\"test_mse\")    # Track RMSE\n",
    "\n",
    "# A single training step function\n",
    "@tf.function\n",
    "def train_step(x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, y_true):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        mol_prediction = model([x_atom, x_bonds, x_atom_index, x_bond_index, x_mask], training=True, doprint=False)\n",
    "        # Compute loss\n",
    "        loss = loss_fn(y_true, mol_prediction)\n",
    "    \n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Apply gradients to update the model's weights\n",
    "    # Print out gradients and variables to debug\n",
    "    #for var, grad in zip(model.trainable_variables, gradients):\n",
    "    #    print(f\"Variable: {var.name}, Gradient: {'None' if grad is None else 'Computed'}\")\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # Update metrics\n",
    "    train_loss.update_state(loss)\n",
    "    train_mse.update_state(y_true, mol_prediction)\n",
    "\n",
    "# A single validation step function\n",
    "@tf.function\n",
    "def val_step(x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, y_true):\n",
    "    # Forward pass (without gradient computation)\n",
    "    print('val')\n",
    "    mol_prediction = model([x_atom, x_bonds, x_atom_index, x_bond_index, x_mask], training=False, doprint=False)\n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_true, mol_prediction)\n",
    "    \n",
    "    # Update metrics\n",
    "    val_loss.update_state(loss)\n",
    "    val_mse.update_state(y_true, mol_prediction)\n",
    "\n",
    "def test_model(test_data):\n",
    "    test_mse.reset_states()  # Reset RMSE metric before evaluating\n",
    "\n",
    "    # Loop over batches in the test dataset\n",
    "    for (x_atom, x_bonds, x_atom_index, x_bond_index, x_mask), y_true in test_data:\n",
    "        # Forward pass (inference)\n",
    "        mol_prediction = model([x_atom, x_bonds, x_atom_index, x_bond_index, x_mask], training=False)\n",
    "\n",
    "        # Update metrics\n",
    "        test_mse.update_state(y_true, mol_prediction)\n",
    "\n",
    "    print(f'Test RMSE: {np.sqrt(test_mse.result().numpy())}')\n",
    "\n",
    "# Now you can call the training function\n",
    "# train_model(train_dataset, val_dataset, epochs=80, callback=cosine_annealing_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "febd11b3-41e3-46ff-9273-b1f6693e4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(train_data, val_data, epochs, callbacks=None):\n",
    "    best_val_mse = float('inf')  # Keep track of the best val MAE\n",
    "\n",
    "    # Prepare each callback if provided\n",
    "    if callbacks is not None:\n",
    "        for callback in callbacks:\n",
    "            callback.model = model\n",
    "            callback.on_train_begin()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        # Reset metrics at the start of each epoch\n",
    "        train_loss.reset_states()\n",
    "        train_mse.reset_states()\n",
    "        val_loss.reset_states()\n",
    "        val_mse.reset_states()\n",
    "\n",
    "        # On epoch begin callback\n",
    "        if callbacks is not None:\n",
    "            for callback in callbacks:\n",
    "                callback.on_epoch_begin(epoch)\n",
    "\n",
    "        # Training loop over batches\n",
    "        for (x_atom, x_bonds, x_atom_index, x_bond_index, x_mask), y_true in train_data:\n",
    "            train_step(x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, y_true)\n",
    "\n",
    "        # Validation loop over batches\n",
    "        for (x_atom, x_bonds, x_atom_index, x_bond_index, x_mask), y_true in val_data:\n",
    "            val_step(x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, y_true)\n",
    "\n",
    "        # On epoch end callback\n",
    "        current_val_mse = val_mse.result().numpy()\n",
    "        if callbacks is not None:\n",
    "            for callback in callbacks:\n",
    "                callback.on_epoch_end(epoch)\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f'Epoch {epoch+1}, '\n",
    "              f'Train Loss: {train_loss.result()}, '\n",
    "              f'Train RMSE: {np.sqrt(train_mse.result().numpy())}, '\n",
    "              f'Val RMSE: {np.sqrt(val_mse.result().numpy())}',\n",
    "              f'time: {time.time()-start}')\n",
    "\n",
    "        # Check if the current validation MAE is the best and save the model\n",
    "        if current_val_mse < best_val_mse:\n",
    "            print(f\"Validation MSE improved from {best_val_mse} to {current_val_mse}. Saving model.\")\n",
    "            best_val_mse = current_val_mse\n",
    "            model.save_weights(checkpoint_filepath)\n",
    "\n",
    "    # End of training callback\n",
    "    if callbacks is not None:\n",
    "        for callback in callbacks:\n",
    "            callback.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ad7d2bc-3eac-47c0-b69b-cb788f0af018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your data into batched datasets\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(((x_atom_train, x_bonds_train, x_atom_index_train, x_bond_index_train, x_mask_train), y_train)).batch(batch_size)\n",
    "val_data = tf.data.Dataset.from_tensor_slices(((x_atom_val, x_bonds_val, x_atom_index_val, x_bond_index_val, x_mask_val), y_val)).batch(batch_size)\n",
    "test_data = tf.data.Dataset.from_tensor_slices(((x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test), y_test)).batch(batch_size)\n",
    "\n",
    "checkpoint_filepath = './best/best_model.h5'\n",
    "\n",
    "# Define the ModelCheckpoint callback to save the model with the best val_mae\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_rmse',  # Monitor the validation MAE\n",
    "    save_best_only=True,  # Save only when the validation MAE improves\n",
    "    mode='min',  # \"min\" mode because we want to minimize MAE\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Initialize the cosine annealing learning rate callback\n",
    "cosine_annealing_callback = CosineAnnealingLR_with_Restart(\n",
    "    T_max=10,  # Adjust T_max based on your needs\n",
    "    T_mult=1,  # Adjust T_mult based on your needs\n",
    "    eta_min=1e-6,  # Minimum learning rate\n",
    "    verbose=1,\n",
    "    lr_reduction_factor=0.95,  # Reduce learning rate at restarts\n",
    "    out_dir=\"./model_snapshots\"  # Directory to save model snapshots\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    #checkpoint_callback,\n",
    "    cosine_annealing_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c066ec7-4c90-4d21-a60c-3e5b413e5797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: CosineAnnealing lr to 0.003162277629598975.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['fingerprint_2/neighbor_fc/kernel:0', 'fingerprint_2/neighbor_fc/bias:0', 'fingerprint_2/atomgru_0/kernel:0', 'fingerprint_2/atomgru_0/recurrent_kernel:0', 'fingerprint_2/atomgru_0/bias:0', 'fingerprint_2/align_0/kernel:0', 'fingerprint_2/align_0/bias:0', 'fingerprint_2/attend_0/kernel:0', 'fingerprint_2/attend_0/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['fingerprint_2/neighbor_fc/kernel:0', 'fingerprint_2/neighbor_fc/bias:0', 'fingerprint_2/atomgru_0/kernel:0', 'fingerprint_2/atomgru_0/recurrent_kernel:0', 'fingerprint_2/atomgru_0/bias:0', 'fingerprint_2/align_0/kernel:0', 'fingerprint_2/align_0/bias:0', 'fingerprint_2/attend_0/kernel:0', 'fingerprint_2/attend_0/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['fingerprint_2/neighbor_fc/kernel:0', 'fingerprint_2/neighbor_fc/bias:0', 'fingerprint_2/atomgru_0/kernel:0', 'fingerprint_2/atomgru_0/recurrent_kernel:0', 'fingerprint_2/atomgru_0/bias:0', 'fingerprint_2/align_0/kernel:0', 'fingerprint_2/align_0/bias:0', 'fingerprint_2/attend_0/kernel:0', 'fingerprint_2/attend_0/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['fingerprint_2/neighbor_fc/kernel:0', 'fingerprint_2/neighbor_fc/bias:0', 'fingerprint_2/atomgru_0/kernel:0', 'fingerprint_2/atomgru_0/recurrent_kernel:0', 'fingerprint_2/atomgru_0/bias:0', 'fingerprint_2/align_0/kernel:0', 'fingerprint_2/align_0/bias:0', 'fingerprint_2/attend_0/kernel:0', 'fingerprint_2/attend_0/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['fingerprint_2/neighbor_fc/kernel:0', 'fingerprint_2/neighbor_fc/bias:0', 'fingerprint_2/atomgru_0/kernel:0', 'fingerprint_2/atomgru_0/recurrent_kernel:0', 'fingerprint_2/atomgru_0/bias:0', 'fingerprint_2/align_0/kernel:0', 'fingerprint_2/align_0/bias:0', 'fingerprint_2/attend_0/kernel:0', 'fingerprint_2/attend_0/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['fingerprint_2/neighbor_fc/kernel:0', 'fingerprint_2/neighbor_fc/bias:0', 'fingerprint_2/atomgru_0/kernel:0', 'fingerprint_2/atomgru_0/recurrent_kernel:0', 'fingerprint_2/atomgru_0/bias:0', 'fingerprint_2/align_0/kernel:0', 'fingerprint_2/align_0/bias:0', 'fingerprint_2/attend_0/kernel:0', 'fingerprint_2/attend_0/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['fingerprint_2/neighbor_fc/kernel:0', 'fingerprint_2/neighbor_fc/bias:0', 'fingerprint_2/atomgru_0/kernel:0', 'fingerprint_2/atomgru_0/recurrent_kernel:0', 'fingerprint_2/atomgru_0/bias:0', 'fingerprint_2/align_0/kernel:0', 'fingerprint_2/align_0/bias:0', 'fingerprint_2/attend_0/kernel:0', 'fingerprint_2/attend_0/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['fingerprint_2/neighbor_fc/kernel:0', 'fingerprint_2/neighbor_fc/bias:0', 'fingerprint_2/atomgru_0/kernel:0', 'fingerprint_2/atomgru_0/recurrent_kernel:0', 'fingerprint_2/atomgru_0/bias:0', 'fingerprint_2/align_0/kernel:0', 'fingerprint_2/align_0/bias:0', 'fingerprint_2/attend_0/kernel:0', 'fingerprint_2/attend_0/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['fingerprint_2/neighbor_fc/kernel:0', 'fingerprint_2/neighbor_fc/bias:0', 'fingerprint_2/atomgru_0/kernel:0', 'fingerprint_2/atomgru_0/recurrent_kernel:0', 'fingerprint_2/atomgru_0/bias:0', 'fingerprint_2/align_0/kernel:0', 'fingerprint_2/align_0/bias:0', 'fingerprint_2/attend_0/kernel:0', 'fingerprint_2/attend_0/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['fingerprint_2/neighbor_fc/kernel:0', 'fingerprint_2/neighbor_fc/bias:0', 'fingerprint_2/atomgru_0/kernel:0', 'fingerprint_2/atomgru_0/recurrent_kernel:0', 'fingerprint_2/atomgru_0/bias:0', 'fingerprint_2/align_0/kernel:0', 'fingerprint_2/align_0/bias:0', 'fingerprint_2/attend_0/kernel:0', 'fingerprint_2/attend_0/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['fingerprint_2/neighbor_fc/kernel:0', 'fingerprint_2/neighbor_fc/bias:0', 'fingerprint_2/atomgru_0/kernel:0', 'fingerprint_2/atomgru_0/recurrent_kernel:0', 'fingerprint_2/atomgru_0/bias:0', 'fingerprint_2/align_0/kernel:0', 'fingerprint_2/align_0/bias:0', 'fingerprint_2/attend_0/kernel:0', 'fingerprint_2/attend_0/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['fingerprint_2/neighbor_fc/kernel:0', 'fingerprint_2/neighbor_fc/bias:0', 'fingerprint_2/atomgru_0/kernel:0', 'fingerprint_2/atomgru_0/recurrent_kernel:0', 'fingerprint_2/atomgru_0/bias:0', 'fingerprint_2/align_0/kernel:0', 'fingerprint_2/align_0/bias:0', 'fingerprint_2/attend_0/kernel:0', 'fingerprint_2/attend_0/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\n",
      "Epoch 1, Train Loss: 9.505282402038574, Train RMSE: 3.135585069656372, Val RMSE: 2.7220170497894287 time: 4.114591836929321\n",
      "Validation MSE improved from inf to 7.409376621246338. Saving model.\n",
      "\n",
      "Epoch 00002: CosineAnnealing lr to 0.0030849156595235887.\n",
      "Epoch 2, Train Loss: 4.54384708404541, Train RMSE: 2.1706650257110596, Val RMSE: 1.8193855285644531 time: 0.519885778427124\n",
      "Validation MSE improved from 7.409376621246338 to 3.3101634979248047. Saving model.\n",
      "\n",
      "Epoch 00003: CosineAnnealing lr to 0.0028604024779409483.\n",
      "Epoch 3, Train Loss: 2.7483673095703125, Train RMSE: 1.6658304929733276, Val RMSE: 1.6147785186767578 time: 0.4812319278717041\n",
      "Validation MSE improved from 3.3101634979248047 to 2.6075098514556885. Saving model.\n",
      "\n",
      "Epoch 00004: CosineAnnealing lr to 0.0025107149993396803.\n",
      "Epoch 4, Train Loss: 2.1879830360412598, Train RMSE: 1.4847526550292969, Val RMSE: 1.3772107362747192 time: 0.473297119140625\n",
      "Validation MSE improved from 2.6075098514556885 to 1.8967094421386719. Saving model.\n",
      "\n",
      "Epoch 00005: CosineAnnealing lr to 0.0020700830705412043.\n",
      "Epoch 5, Train Loss: 1.585174560546875, Train RMSE: 1.266036868095398, Val RMSE: 1.1094269752502441 time: 0.4777390956878662\n",
      "Validation MSE improved from 1.8967094421386719 to 1.2308281660079956. Saving model.\n",
      "\n",
      "Epoch 00006: CosineAnnealing lr to 0.0015816388147994874.\n",
      "Epoch 6, Train Loss: 1.271854043006897, Train RMSE: 1.1386383771896362, Val RMSE: 1.1141307353973389 time: 0.4919748306274414\n",
      "\n",
      "Epoch 00007: CosineAnnealing lr to 0.0010931945590577708.\n",
      "Epoch 7, Train Loss: 1.1521589756011963, Train RMSE: 1.079938530921936, Val RMSE: 1.0220255851745605 time: 0.48172903060913086\n",
      "Validation MSE improved from 1.2308281660079956 to 1.0445363521575928. Saving model.\n",
      "\n",
      "Epoch 00008: CosineAnnealing lr to 0.0006525626302592953.\n",
      "Epoch 8, Train Loss: 0.9417049288749695, Train RMSE: 0.9764213562011719, Val RMSE: 0.937811017036438 time: 0.4731748104095459\n",
      "Validation MSE improved from 1.0445363521575928 to 0.879489541053772. Saving model.\n",
      "\n",
      "Epoch 00009: CosineAnnealing lr to 0.0003028751516580271.\n",
      "Epoch 9, Train Loss: 0.8285548090934753, Train RMSE: 0.9194095134735107, Val RMSE: 0.9024609923362732 time: 0.4763188362121582\n",
      "Validation MSE improved from 0.879489541053772 to 0.8144358396530151. Saving model.\n",
      "\n",
      "Epoch 00010: CosineAnnealing lr to 7.836197007538655e-05.\n",
      "Restart at epoch 00010\n",
      "Epoch 10, Train Loss: 0.7327898740768433, Train RMSE: 0.8667331337928772, Val RMSE: 0.8690512180328369 time: 0.502068042755127\n",
      "Validation MSE improved from 0.8144358396530151 to 0.7552499771118164. Saving model.\n",
      "\n",
      "Epoch 00011: CosineAnnealing lr to 0.003004163748119026.\n",
      "Epoch 11, Train Loss: 0.6343785524368286, Train RMSE: 0.8027568459510803, Val RMSE: 0.8523334264755249 time: 0.47588086128234863\n",
      "Validation MSE improved from 0.7552499771118164 to 0.7264722585678101. Saving model.\n",
      "\n",
      "Epoch 00012: CosineAnnealing lr to 0.0029306711001345015.\n",
      "Epoch 12, Train Loss: 0.5678130984306335, Train RMSE: 0.7562628388404846, Val RMSE: 0.8282269835472107 time: 0.4750678539276123\n",
      "Validation MSE improved from 0.7264722585678101 to 0.6859599351882935. Saving model.\n",
      "\n",
      "Epoch 00013: CosineAnnealing lr to 0.002717387128619041.\n",
      "Epoch 13, Train Loss: 0.5288351774215698, Train RMSE: 0.730299711227417, Val RMSE: 0.7987666130065918 time: 0.4766499996185303\n",
      "Validation MSE improved from 0.6859599351882935 to 0.6380281448364258. Saving model.\n",
      "\n",
      "Epoch 00014: CosineAnnealing lr to 0.0023851895547413885.\n",
      "Epoch 14, Train Loss: 0.4987361431121826, Train RMSE: 0.7086401581764221, Val RMSE: 0.7797943949699402 time: 0.4826509952545166\n",
      "Validation MSE improved from 0.6380281448364258 to 0.6080793142318726. Saving model.\n",
      "\n",
      "Epoch 00015: CosineAnnealing lr to 0.001966596191589285.\n",
      "Epoch 15, Train Loss: 0.47619056701660156, Train RMSE: 0.6937574744224548, Val RMSE: 0.7664768695831299 time: 0.4794459342956543\n",
      "Validation MSE improved from 0.6080793142318726 to 0.5874868035316467. Saving model.\n",
      "\n",
      "Epoch 00016: CosineAnnealing lr to 0.001502581874059513.\n",
      "Epoch 16, Train Loss: 0.45708709955215454, Train RMSE: 0.6800535321235657, Val RMSE: 0.7567957639694214 time: 0.4789268970489502\n",
      "Validation MSE improved from 0.5874868035316467 to 0.572739839553833. Saving model.\n",
      "\n",
      "Epoch 00017: CosineAnnealing lr to 0.0010385675565297416.\n",
      "Epoch 17, Train Loss: 0.43425336480140686, Train RMSE: 0.6637638807296753, Val RMSE: 0.7525361776351929 time: 0.47327685356140137\n",
      "Validation MSE improved from 0.572739839553833 to 0.566310703754425. Saving model.\n",
      "\n",
      "Epoch 00018: CosineAnnealing lr to 0.0006199741933776377.\n",
      "Epoch 18, Train Loss: 0.41300448775291443, Train RMSE: 0.647571325302124, Val RMSE: 0.7458829283714294 time: 0.4776580333709717\n",
      "Validation MSE improved from 0.566310703754425 to 0.5563413500785828. Saving model.\n",
      "\n",
      "Epoch 00019: CosineAnnealing lr to 0.0002877766194999851.\n",
      "Epoch 19, Train Loss: 0.3871641755104065, Train RMSE: 0.627906084060669, Val RMSE: 0.7336133718490601 time: 0.47994065284729004\n",
      "Validation MSE improved from 0.5563413500785828 to 0.5381885766983032. Saving model.\n",
      "\n",
      "Epoch 00020: CosineAnnealing lr to 7.449264798452459e-05.\n",
      "Restart at epoch 00020\n",
      "Epoch 20, Train Loss: 0.35809624195098877, Train RMSE: 0.6052714586257935, Val RMSE: 0.7240142226219177 time: 0.4969000816345215\n",
      "Validation MSE improved from 0.5381885766983032 to 0.5241965651512146. Saving model.\n",
      "\n",
      "Epoch 00021: CosineAnnealing lr to 0.0028539555607130745.\n",
      "Epoch 21, Train Loss: 0.3307379186153412, Train RMSE: 0.583660900592804, Val RMSE: 0.716185986995697 time: 0.47911691665649414\n",
      "Validation MSE improved from 0.5241965651512146 to 0.5129223465919495. Saving model.\n",
      "\n",
      "Epoch 00022: CosineAnnealing lr to 0.002784138768714869.\n",
      "Epoch 22, Train Loss: 0.3074035048484802, Train RMSE: 0.563615083694458, Val RMSE: 0.710414469242096 time: 0.48424386978149414\n",
      "Validation MSE improved from 0.5129223465919495 to 0.5046887397766113. Saving model.\n",
      "\n",
      "Epoch 00023: CosineAnnealing lr to 0.0025815225467632295.\n",
      "Epoch 23, Train Loss: 0.29068249464035034, Train RMSE: 0.5493891835212708, Val RMSE: 0.6971989274024963 time: 0.47713804244995117\n",
      "Validation MSE improved from 0.5046887397766113 to 0.48608633875846863. Saving model.\n",
      "\n",
      "Epoch 00024: CosineAnnealing lr to 0.0022659403823730114.\n",
      "Epoch 24, Train Loss: 0.2700954079627991, Train RMSE: 0.5305092930793762, Val RMSE: 0.689437747001648 time: 0.4740729331970215\n",
      "Validation MSE improved from 0.48608633875846863 to 0.4753243923187256. Saving model.\n",
      "\n",
      "Epoch 00025: CosineAnnealing lr to 0.0018682836565849608.\n",
      "Epoch 25, Train Loss: 0.2544902265071869, Train RMSE: 0.5157474279403687, Val RMSE: 0.6756431460380554 time: 0.47653818130493164\n",
      "Validation MSE improved from 0.4753243923187256 to 0.45649367570877075. Saving model.\n",
      "\n",
      "Epoch 00026: CosineAnnealing lr to 0.0014274777803565371.\n",
      "Epoch 26, Train Loss: 0.23825626075267792, Train RMSE: 0.5000925660133362, Val RMSE: 0.6717758774757385 time: 0.47499608993530273\n",
      "Validation MSE improved from 0.45649367570877075 to 0.451282799243927. Saving model.\n",
      "\n",
      "Epoch 00027: CosineAnnealing lr to 0.0009866719041281137.\n",
      "Epoch 27, Train Loss: 0.22529490292072296, Train RMSE: 0.48712971806526184, Val RMSE: 0.6653189659118652 time: 0.46856021881103516\n",
      "Validation MSE improved from 0.451282799243927 to 0.44264930486679077. Saving model.\n",
      "\n",
      "Epoch 00028: CosineAnnealing lr to 0.0005890151783400631.\n",
      "Epoch 28, Train Loss: 0.20944175124168396, Train RMSE: 0.4703649878501892, Val RMSE: 0.6581747531890869 time: 0.47361087799072266\n",
      "Validation MSE improved from 0.44264930486679077 to 0.43319404125213623. Saving model.\n",
      "\n",
      "Epoch 00029: CosineAnnealing lr to 0.0002734330139498452.\n",
      "Epoch 29, Train Loss: 0.1946638971567154, Train RMSE: 0.4542892873287201, Val RMSE: 0.6562124490737915 time: 0.4745650291442871\n",
      "Validation MSE improved from 0.43319404125213623 to 0.43061476945877075. Saving model.\n",
      "\n",
      "Epoch 00030: CosineAnnealing lr to 7.081679199820574e-05.\n",
      "Restart at epoch 00030\n",
      "Epoch 30, Train Loss: 0.18253587186336517, Train RMSE: 0.44051501154899597, Val RMSE: 0.6507276296615601 time: 0.49280309677124023\n",
      "Validation MSE improved from 0.43061476945877075 to 0.4234464168548584. Saving model.\n",
      "\n",
      "Epoch 00031: CosineAnnealing lr to 0.0027112577826774206.\n",
      "Epoch 31, Train Loss: 0.17267677187919617, Train RMSE: 0.4276527464389801, Val RMSE: 0.6897399425506592 time: 0.4673151969909668\n",
      "\n",
      "Epoch 00032: CosineAnnealing lr to 0.002644933053866218.\n",
      "Epoch 32, Train Loss: 0.16777101159095764, Train RMSE: 0.42359447479248047, Val RMSE: 0.6275508403778076 time: 0.467792272567749\n",
      "Validation MSE improved from 0.4234464168548584 to 0.39382004737854004. Saving model.\n",
      "\n",
      "Epoch 00033: CosineAnnealing lr to 0.0024524511940002083.\n",
      "Epoch 33, Train Loss: 0.2993289530277252, Train RMSE: 0.5483869910240173, Val RMSE: 0.6865261793136597 time: 0.4698159694671631\n",
      "\n",
      "Epoch 00034: CosineAnnealing lr to 0.0021526536686230537.\n",
      "Epoch 34, Train Loss: 0.32334041595458984, Train RMSE: 0.5647836327552795, Val RMSE: 0.7200270295143127 time: 0.5702848434448242\n",
      "\n",
      "Epoch 00035: CosineAnnealing lr to 0.001774886748330853.\n",
      "Epoch 35, Train Loss: 0.2512008547782898, Train RMSE: 0.5108838081359863, Val RMSE: 0.6521326303482056 time: 0.4752180576324463\n",
      "\n",
      "Epoch 00036: CosineAnnealing lr to 0.0013561288913387102.\n",
      "Epoch 36, Train Loss: 0.17157241702079773, Train RMSE: 0.4259973466396332, Val RMSE: 0.6776774525642395 time: 0.4806978702545166\n",
      "\n",
      "Epoch 00037: CosineAnnealing lr to 0.0009373710343465675.\n",
      "Epoch 37, Train Loss: 0.1735866516828537, Train RMSE: 0.43018174171447754, Val RMSE: 0.7545090913772583 time: 0.4675321578979492\n",
      "\n",
      "Epoch 00038: CosineAnnealing lr to 0.0005596041140543672.\n",
      "Epoch 38, Train Loss: 0.20415592193603516, Train RMSE: 0.4620538651943207, Val RMSE: 0.7721337676048279 time: 0.4709601402282715\n",
      "\n",
      "Epoch 00039: CosineAnnealing lr to 0.0002598065886772123.\n",
      "Epoch 39, Train Loss: 0.19073481857776642, Train RMSE: 0.4476102888584137, Val RMSE: 0.8078888654708862 time: 0.48515987396240234\n",
      "\n",
      "Epoch 00040: CosineAnnealing lr to 6.732472881120283e-05.\n",
      "Restart at epoch 00040\n",
      "Epoch 40, Train Loss: 0.21697518229484558, Train RMSE: 0.4807548522949219, Val RMSE: 0.9293144941329956 time: 0.5010800361633301\n",
      "\n",
      "Epoch 00041: CosineAnnealing lr to 0.0025756948935435494.\n",
      "Epoch 41, Train Loss: 0.28507938981056213, Train RMSE: 0.5577605962753296, Val RMSE: 1.1076717376708984 time: 0.4789278507232666\n",
      "\n",
      "Epoch 00042: CosineAnnealing lr to 0.0025126876247599994.\n",
      "Epoch 42, Train Loss: 0.47298145294189453, Train RMSE: 0.704224169254303, Val RMSE: 0.9476947784423828 time: 0.47613096237182617\n",
      "\n",
      "Epoch 00043: CosineAnnealing lr to 0.0023298334088753384.\n",
      "Epoch 43, Train Loss: 0.7581961750984192, Train RMSE: 0.8441037535667419, Val RMSE: 0.7119001746177673 time: 0.4741089344024658\n",
      "\n",
      "Epoch 00044: CosineAnnealing lr to 0.0020450312905605935.\n",
      "Epoch 44, Train Loss: 0.6637464761734009, Train RMSE: 0.8471035361289978, Val RMSE: 0.9974914193153381 time: 0.4757368564605713\n",
      "\n",
      "Epoch 00045: CosineAnnealing lr to 0.001686159685489451.\n",
      "Epoch 45, Train Loss: 0.5332768559455872, Train RMSE: 0.744422972202301, Val RMSE: 0.7389875650405884 time: 0.4724290370941162\n",
      "\n",
      "Epoch 00046: CosineAnnealing lr to 0.0012883474467717745.\n",
      "Epoch 46, Train Loss: 0.2411811351776123, Train RMSE: 0.5034041404724121, Val RMSE: 0.699777364730835 time: 0.4793572425842285\n",
      "\n",
      "Epoch 00047: CosineAnnealing lr to 0.0008905352080540984.\n",
      "Epoch 47, Train Loss: 0.24231533706188202, Train RMSE: 0.5015210509300232, Val RMSE: 0.769981861114502 time: 0.49121999740600586\n",
      "\n",
      "Epoch 00048: CosineAnnealing lr to 0.0005316636029829561.\n",
      "Epoch 48, Train Loss: 0.24339547753334045, Train RMSE: 0.5054535269737244, Val RMSE: 0.6749621033668518 time: 0.489764928817749\n",
      "\n",
      "Epoch 00049: CosineAnnealing lr to 0.000246861484668211.\n",
      "Epoch 49, Train Loss: 0.1524260938167572, Train RMSE: 0.4023362100124359, Val RMSE: 0.6723603010177612 time: 0.48322081565856934\n",
      "\n",
      "Epoch 00050: CosineAnnealing lr to 6.400726878355005e-05.\n",
      "Restart at epoch 00050\n",
      "Epoch 50, Train Loss: 0.1483164131641388, Train RMSE: 0.39635300636291504, Val RMSE: 0.7024685740470886 time: 0.49632787704467773\n",
      "\n",
      "Epoch 00051: CosineAnnealing lr to 0.002446910148866372.\n",
      "Epoch 51, Train Loss: 0.15195982158184052, Train RMSE: 0.4039508104324341, Val RMSE: 0.6665284633636475 time: 0.46984291076660156\n",
      "\n",
      "Epoch 00052: CosineAnnealing lr to 0.0023870544671090923.\n",
      "Epoch 52, Train Loss: 0.11285444349050522, Train RMSE: 0.34813404083251953, Val RMSE: 0.6542930006980896 time: 0.4667627811431885\n",
      "\n",
      "Epoch 00053: CosineAnnealing lr to 0.0022133465130067123.\n",
      "Epoch 53, Train Loss: 0.10370872169733047, Train RMSE: 0.33195534348487854, Val RMSE: 0.6593440771102905 time: 0.4758462905883789\n",
      "\n",
      "Epoch 00054: CosineAnnealing lr to 0.0019427900314012564.\n",
      "Epoch 54, Train Loss: 0.1021457090973854, Train RMSE: 0.3317442536354065, Val RMSE: 0.6577690839767456 time: 0.46944093704223633\n",
      "\n",
      "Epoch 00055: CosineAnnealing lr to 0.001601868975790119.\n",
      "Epoch 55, Train Loss: 0.09402044117450714, Train RMSE: 0.31869518756866455, Val RMSE: 0.6443217992782593 time: 0.4717752933502197\n",
      "\n",
      "Epoch 00056: CosineAnnealing lr to 0.0012239550744331858.\n",
      "Epoch 56, Train Loss: 0.0806460753083229, Train RMSE: 0.29480981826782227, Val RMSE: 0.6452827453613281 time: 0.4691760540008545\n",
      "\n",
      "Epoch 00057: CosineAnnealing lr to 0.0008460411730762528.\n",
      "Epoch 57, Train Loss: 0.07683028280735016, Train RMSE: 0.2870374619960785, Val RMSE: 0.6435999274253845 time: 0.46961307525634766\n",
      "\n",
      "Epoch 00058: CosineAnnealing lr to 0.0005051201174651156.\n",
      "Epoch 58, Train Loss: 0.0747963935136795, Train RMSE: 0.2835032045841217, Val RMSE: 0.6470034718513489 time: 0.4802889823913574\n",
      "\n",
      "Epoch 00059: CosineAnnealing lr to 0.00023456363585965984.\n",
      "Epoch 59, Train Loss: 0.0719972625374794, Train RMSE: 0.27906954288482666, Val RMSE: 0.6482889652252197 time: 0.4710097312927246\n",
      "\n",
      "Epoch 00060: CosineAnnealing lr to 6.0855681757279936e-05.\n",
      "Restart at epoch 00060\n",
      "Epoch 60, Train Loss: 0.06680817902088165, Train RMSE: 0.2690644860267639, Val RMSE: 0.6445008516311646 time: 0.5048961639404297\n",
      "\n",
      "Epoch 00061: CosineAnnealing lr to 0.0023245646414230534.\n",
      "Epoch 61, Train Loss: 0.060581643134355545, Train RMSE: 0.2561646103858948, Val RMSE: 0.6446946859359741 time: 0.4692568778991699\n",
      "\n",
      "Epoch 00062: CosineAnnealing lr to 0.0022677029673407304.\n",
      "Epoch 62, Train Loss: 0.05506882816553116, Train RMSE: 0.24429772794246674, Val RMSE: 0.6444721817970276 time: 0.46845316886901855\n",
      "\n",
      "Epoch 00063: CosineAnnealing lr to 0.0021026839619315172.\n",
      "Epoch 63, Train Loss: 0.05179295688867569, Train RMSE: 0.23651424050331116, Val RMSE: 0.6435286998748779 time: 0.467785120010376\n",
      "\n",
      "Epoch 00064: CosineAnnealing lr to 0.0018456608351998863.\n",
      "Epoch 64, Train Loss: 0.05082617327570915, Train RMSE: 0.23355630040168762, Val RMSE: 0.6403753757476807 time: 0.4727802276611328\n",
      "\n",
      "Epoch 00065: CosineAnnealing lr to 0.0015217928015757538.\n",
      "Epoch 65, Train Loss: 0.05080462619662285, Train RMSE: 0.2330416589975357, Val RMSE: 0.6397382616996765 time: 0.46916818618774414\n",
      "\n",
      "Epoch 00066: CosineAnnealing lr to 0.0011627823207115266.\n",
      "Epoch 66, Train Loss: 0.05101511627435684, Train RMSE: 0.23401713371276855, Val RMSE: 0.6457077860832214 time: 0.4694557189941406\n",
      "\n",
      "Epoch 00067: CosineAnnealing lr to 0.0008037718398472996.\n",
      "Epoch 67, Train Loss: 0.05359319970011711, Train RMSE: 0.24124959111213684, Val RMSE: 0.6595802307128906 time: 0.4824960231781006\n",
      "\n",
      "Epoch 00068: CosineAnnealing lr to 0.00047990380622316717.\n",
      "Epoch 68, Train Loss: 0.060869522392749786, Train RMSE: 0.2576150596141815, Val RMSE: 0.670760452747345 time: 0.4887220859527588\n",
      "\n",
      "Epoch 00069: CosineAnnealing lr to 0.00022288067949153623.\n",
      "Epoch 69, Train Loss: 0.07164089381694794, Train RMSE: 0.2757483720779419, Val RMSE: 0.6646032333374023 time: 0.48850202560424805\n",
      "\n",
      "Epoch 00070: CosineAnnealing lr to 5.786167408232332e-05.\n",
      "Restart at epoch 00070\n",
      "Epoch 70, Train Loss: 0.07905389368534088, Train RMSE: 0.28228095173835754, Val RMSE: 0.6684755682945251 time: 0.5165619850158691\n",
      "\n",
      "Epoch 00071: CosineAnnealing lr to 0.002208336409351901.\n",
      "Epoch 71, Train Loss: 0.07956255972385406, Train RMSE: 0.28384754061698914, Val RMSE: 0.7685467600822449 time: 0.4821920394897461\n",
      "\n",
      "Epoch 00072: CosineAnnealing lr to 0.0021543190425607865.\n",
      "Epoch 72, Train Loss: 0.1106620579957962, Train RMSE: 0.3461691439151764, Val RMSE: 0.8232305645942688 time: 0.4833219051361084\n",
      "\n",
      "Epoch 00073: CosineAnnealing lr to 0.001997554538410082.\n",
      "Epoch 73, Train Loss: 0.2007150948047638, Train RMSE: 0.44592589139938354, Val RMSE: 0.6616424918174744 time: 0.47819995880126953\n",
      "\n",
      "Epoch 00074: CosineAnnealing lr to 0.0017533880988085846.\n",
      "Epoch 74, Train Loss: 0.20772483944892883, Train RMSE: 0.44725534319877625, Val RMSE: 0.7507759928703308 time: 0.4729349613189697\n",
      "\n",
      "Epoch 00075: CosineAnnealing lr to 0.0014457204360721068.\n",
      "Epoch 75, Train Loss: 0.19836214184761047, Train RMSE: 0.4650319218635559, Val RMSE: 0.7296815514564514 time: 0.47722291946411133\n",
      "\n",
      "Epoch 00076: CosineAnnealing lr to 0.0011046682046759503.\n",
      "Epoch 76, Train Loss: 0.16783401370048523, Train RMSE: 0.4110155701637268, Val RMSE: 0.6807147264480591 time: 0.47220706939697266\n",
      "\n",
      "Epoch 00077: CosineAnnealing lr to 0.000763615973279794.\n",
      "Epoch 77, Train Loss: 0.08792515099048615, Train RMSE: 0.30860668420791626, Val RMSE: 0.7022960186004639 time: 0.47063183784484863\n",
      "\n",
      "Epoch 00078: CosineAnnealing lr to 0.0004559483105433161.\n",
      "Epoch 78, Train Loss: 0.08130538463592529, Train RMSE: 0.28809407353401184, Val RMSE: 0.6478269696235657 time: 0.4699561595916748\n",
      "\n",
      "Epoch 00079: CosineAnnealing lr to 0.0002117818709418188.\n",
      "Epoch 79, Train Loss: 0.06348182260990143, Train RMSE: 0.2629930377006531, Val RMSE: 0.6579259634017944 time: 0.46948885917663574\n",
      "\n",
      "Epoch 00080: CosineAnnealing lr to 5.501736679111453e-05.\n",
      "Restart at epoch 00080\n",
      "Epoch 80, Train Loss: 0.055775366723537445, Train RMSE: 0.24199917912483215, Val RMSE: 0.6640299558639526 time: 0.49818897247314453\n",
      "\n",
      "Epoch 00081: CosineAnnealing lr to 0.0020979195888843056.\n",
      "Epoch 81, Train Loss: 0.038920577615499496, Train RMSE: 0.2054809033870697, Val RMSE: 0.6612080931663513 time: 0.4736518859863281\n",
      "\n",
      "Epoch 00082: CosineAnnealing lr to 0.0020466043140198397.\n",
      "Epoch 82, Train Loss: 0.03895768150687218, Train RMSE: 0.20206710696220398, Val RMSE: 0.6340137124061584 time: 0.47035884857177734\n",
      "\n",
      "Epoch 00083: CosineAnnealing lr to 0.0018976815860647183.\n",
      "Epoch 83, Train Loss: 0.03529276326298714, Train RMSE: 0.19471046328544617, Val RMSE: 0.6417456865310669 time: 0.4674251079559326\n",
      "\n",
      "Epoch 00084: CosineAnnealing lr to 0.001665728999236848.\n",
      "Epoch 84, Train Loss: 0.03393876925110817, Train RMSE: 0.19214481115341187, Val RMSE: 0.6440301537513733 time: 0.4784376621246338\n",
      "\n",
      "Epoch 00085: CosineAnnealing lr to 0.001373451688843642.\n",
      "Epoch 85, Train Loss: 0.028559137135744095, Train RMSE: 0.17518407106399536, Val RMSE: 0.64984530210495 time: 0.47209715843200684\n",
      "\n",
      "Epoch 00086: CosineAnnealing lr to 0.0010494597944421527.\n",
      "Epoch 86, Train Loss: 0.025017783045768738, Train RMSE: 0.164875328540802, Val RMSE: 0.6529073119163513 time: 0.4685230255126953\n",
      "\n",
      "Epoch 00087: CosineAnnealing lr to 0.0007254679000406636.\n",
      "Epoch 87, Train Loss: 0.024494729936122894, Train RMSE: 0.1623528152704239, Val RMSE: 0.6465601325035095 time: 0.46889495849609375\n",
      "\n",
      "Epoch 00088: CosineAnnealing lr to 0.0004331905896474576.\n",
      "Epoch 88, Train Loss: 0.024130970239639282, Train RMSE: 0.16019576787948608, Val RMSE: 0.6393261551856995 time: 0.4711778163909912\n",
      "\n",
      "Epoch 00089: CosineAnnealing lr to 0.00020123800281958722.\n",
      "Epoch 89, Train Loss: 0.023664742708206177, Train RMSE: 0.16019122302532196, Val RMSE: 0.6395657062530518 time: 0.4708259105682373\n",
      "\n",
      "Epoch 00090: CosineAnnealing lr to 5.231527486446618e-05.\n",
      "Restart at epoch 00090\n",
      "Epoch 90, Train Loss: 0.024882223457098007, Train RMSE: 0.16464178264141083, Val RMSE: 0.6391752362251282 time: 0.4938180446624756\n",
      "\n",
      "Epoch 00091: CosineAnnealing lr to 0.0019930236094400904.\n",
      "Epoch 91, Train Loss: 0.024668794125318527, Train RMSE: 0.16181935369968414, Val RMSE: 0.6446118950843811 time: 0.47217369079589844\n",
      "\n",
      "Epoch 00092: CosineAnnealing lr to 0.0019442753219059401.\n",
      "Epoch 92, Train Loss: 0.021978823468089104, Train RMSE: 0.1523795872926712, Val RMSE: 0.6630405187606812 time: 0.4756760597229004\n",
      "\n",
      "Epoch 00093: CosineAnnealing lr to 0.001802802281336623.\n",
      "Epoch 93, Train Loss: 0.02217046357691288, Train RMSE: 0.15518181025981903, Val RMSE: 0.6780651211738586 time: 0.4720942974090576\n",
      "\n",
      "Epoch 00094: CosineAnnealing lr to 0.0015824528546436983.\n",
      "Epoch 94, Train Loss: 0.02880055084824562, Train RMSE: 0.17438268661499023, Val RMSE: 0.6590985655784607 time: 0.46939587593078613\n",
      "\n",
      "Epoch 00095: CosineAnnealing lr to 0.0013047963789766005.\n",
      "Epoch 95, Train Loss: 0.03516238182783127, Train RMSE: 0.18611355125904083, Val RMSE: 0.6288109421730042 time: 0.47472119331359863\n",
      "\n",
      "Epoch 00096: CosineAnnealing lr to 0.000997011804720045.\n",
      "Epoch 96, Train Loss: 0.03765787556767464, Train RMSE: 0.19487158954143524, Val RMSE: 0.6501559019088745 time: 0.4748091697692871\n",
      "\n",
      "Epoch 00097: CosineAnnealing lr to 0.0006892272304634898.\n",
      "Epoch 97, Train Loss: 0.05482124537229538, Train RMSE: 0.24487502872943878, Val RMSE: 0.6927825212478638 time: 0.46884679794311523\n",
      "\n",
      "Epoch 00098: CosineAnnealing lr to 0.00041157075479639205.\n",
      "Epoch 98, Train Loss: 0.08937977254390717, Train RMSE: 0.30353549122810364, Val RMSE: 0.6581991910934448 time: 0.46884989738464355\n",
      "\n",
      "Epoch 00099: CosineAnnealing lr to 0.00019122132810346724.\n",
      "Epoch 99, Train Loss: 0.08848259598016739, Train RMSE: 0.2908135950565338, Val RMSE: 0.7120492458343506 time: 0.47293686866760254\n",
      "\n",
      "Epoch 00100: CosineAnnealing lr to 4.974828753415025e-05.\n",
      "Restart at epoch 00100\n",
      "Epoch 100, Train Loss: 0.08151685446500778, Train RMSE: 0.2944897413253784, Val RMSE: 0.8262411952018738 time: 0.5003209114074707\n",
      "\n",
      "Epoch 00101: CosineAnnealing lr to 0.0018933724289680858.\n",
      "Epoch 101, Train Loss: 0.1332557648420334, Train RMSE: 0.3703332245349884, Val RMSE: 0.6911643743515015 time: 0.4797172546386719\n",
      "\n",
      "Epoch 00102: CosineAnnealing lr to 0.0018470627793977358.\n",
      "Epoch 102, Train Loss: 0.12136019766330719, Train RMSE: 0.3398420214653015, Val RMSE: 0.6722350716590881 time: 0.47033214569091797\n",
      "\n",
      "Epoch 00103: CosineAnnealing lr to 0.0017126669418449326.\n",
      "Epoch 103, Train Loss: 0.09407461434602737, Train RMSE: 0.32118967175483704, Val RMSE: 0.6937193274497986 time: 0.4690709114074707\n",
      "\n",
      "Epoch 00104: CosineAnnealing lr to 0.0015033405172802062.\n",
      "Epoch 104, Train Loss: 0.09204787760972977, Train RMSE: 0.30456966161727905, Val RMSE: 0.6514264345169067 time: 0.4688718318939209\n",
      "\n",
      "Epoch 00105: CosineAnnealing lr to 0.0012395738346029112.\n",
      "Epoch 105, Train Loss: 0.046001505106687546, Train RMSE: 0.22194673120975494, Val RMSE: 0.6986472010612488 time: 0.47347402572631836\n",
      "\n",
      "Epoch 00106: CosineAnnealing lr to 0.000947186214484043.\n",
      "Epoch 106, Train Loss: 0.04540129750967026, Train RMSE: 0.21565009653568268, Val RMSE: 0.6471813917160034 time: 0.4716298580169678\n",
      "\n",
      "Epoch 00107: CosineAnnealing lr to 0.0006547985943651748.\n",
      "Epoch 107, Train Loss: 0.029853373765945435, Train RMSE: 0.17773665487766266, Val RMSE: 0.6584464907646179 time: 0.4690260887145996\n",
      "\n",
      "Epoch 00108: CosineAnnealing lr to 0.0003910319116878798.\n",
      "Epoch 108, Train Loss: 0.029745588079094887, Train RMSE: 0.1780495047569275, Val RMSE: 0.6445882320404053 time: 0.46730875968933105\n",
      "\n",
      "Epoch 00109: CosineAnnealing lr to 0.00018170548712315325.\n",
      "Epoch 109, Train Loss: 0.02077171579003334, Train RMSE: 0.14731886982917786, Val RMSE: 0.6563001871109009 time: 0.4722459316253662\n",
      "\n",
      "Epoch 00110: CosineAnnealing lr to 4.730964957035012e-05.\n",
      "Restart at epoch 00110\n",
      "Epoch 110, Train Loss: 0.018898243084549904, Train RMSE: 0.14274433255195618, Val RMSE: 0.6547029614448547 time: 0.4951770305633545\n",
      "\n",
      "Epoch 00111: CosineAnnealing lr to 0.0017987038075196814.\n",
      "Epoch 111, Train Loss: 0.01903027668595314, Train RMSE: 0.1397055834531784, Val RMSE: 0.6414833664894104 time: 0.47109198570251465\n",
      "\n",
      "Epoch 00112: CosineAnnealing lr to 0.0017547108640149414.\n",
      "Epoch 112, Train Loss: 0.01646803691983223, Train RMSE: 0.13310836255550385, Val RMSE: 0.6464487910270691 time: 0.4698460102081299\n",
      "\n",
      "Epoch 00113: CosineAnnealing lr to 0.0016270383693278265.\n",
      "Epoch 113, Train Loss: 0.017451155930757523, Train RMSE: 0.1370202898979187, Val RMSE: 0.6428719162940979 time: 0.46977877616882324\n",
      "\n",
      "Epoch 00114: CosineAnnealing lr to 0.0014281837967848885.\n",
      "Epoch 114, Train Loss: 0.014951678924262524, Train RMSE: 0.12530678510665894, Val RMSE: 0.6495029330253601 time: 0.4730548858642578\n",
      "\n",
      "Epoch 00115: CosineAnnealing lr to 0.001177612417447906.\n",
      "Epoch 115, Train Loss: 0.012353789992630482, Train RMSE: 0.11581537127494812, Val RMSE: 0.6606626510620117 time: 0.4722461700439453\n",
      "\n",
      "Epoch 00116: CosineAnnealing lr to 0.0008998519037598407.\n",
      "Epoch 116, Train Loss: 0.013144311495125294, Train RMSE: 0.11843237280845642, Val RMSE: 0.651202917098999 time: 0.4721190929412842\n",
      "\n",
      "Epoch 00117: CosineAnnealing lr to 0.0006220913900717753.\n",
      "Epoch 117, Train Loss: 0.012406074441969395, Train RMSE: 0.11343955993652344, Val RMSE: 0.6403948664665222 time: 0.46837615966796875\n",
      "\n",
      "Epoch 00118: CosineAnnealing lr to 0.00037152001073479307.\n",
      "Epoch 118, Train Loss: 0.012130805291235447, Train RMSE: 0.1147005632519722, Val RMSE: 0.638727068901062 time: 0.4695560932159424\n",
      "\n",
      "Epoch 00119: CosineAnnealing lr to 0.00017266543819185494.\n",
      "Epoch 119, Train Loss: 0.013906566426157951, Train RMSE: 0.12261045724153519, Val RMSE: 0.6378749012947083 time: 0.46921277046203613\n",
      "\n",
      "Epoch 00120: CosineAnnealing lr to 4.4992943504739986e-05.\n",
      "Restart at epoch 00120\n",
      "Epoch 120, Train Loss: 0.013219313696026802, Train RMSE: 0.11697067320346832, Val RMSE: 0.6489167809486389 time: 0.4946470260620117\n",
      "\n",
      "Epoch 00121: CosineAnnealing lr to 0.0017087686171436973.\n",
      "Epoch 121, Train Loss: 0.011144518852233887, Train RMSE: 0.10872873663902283, Val RMSE: 0.6681906580924988 time: 0.4755089282989502\n",
      "\n",
      "Epoch 00122: CosineAnnealing lr to 0.001666976544401287.\n",
      "Epoch 122, Train Loss: 0.013138772919774055, Train RMSE: 0.11950299143791199, Val RMSE: 0.6680585741996765 time: 0.4694352149963379\n",
      "\n",
      "Epoch 00123: CosineAnnealing lr to 0.0015456912254365757.\n",
      "Epoch 123, Train Loss: 0.015915852040052414, Train RMSE: 0.127670556306839, Val RMSE: 0.6445291638374329 time: 0.4750251770019531\n",
      "\n",
      "Epoch 00124: CosineAnnealing lr to 0.0013567849123143367.\n",
      "Epoch 124, Train Loss: 0.015246175229549408, Train RMSE: 0.12278052419424057, Val RMSE: 0.633969783782959 time: 0.47087812423706055\n",
      "\n",
      "Epoch 00125: CosineAnnealing lr to 0.0011187490711506514.\n",
      "Epoch 125, Train Loss: 0.018203234300017357, Train RMSE: 0.14026573300361633, Val RMSE: 0.6463994979858398 time: 0.47444581985473633\n",
      "\n",
      "Epoch 00126: CosineAnnealing lr to 0.0008548843085718487.\n",
      "Epoch 126, Train Loss: 0.028776466846466064, Train RMSE: 0.175477534532547, Val RMSE: 0.6456442475318909 time: 0.47258996963500977\n",
      "\n",
      "Epoch 00127: CosineAnnealing lr to 0.0005910195459930459.\n",
      "Epoch 127, Train Loss: 0.031258899718523026, Train RMSE: 0.1752823293209076, Val RMSE: 0.6570205688476562 time: 0.47327184677124023\n",
      "\n",
      "Epoch 00128: CosineAnnealing lr to 0.00035298370482936075.\n",
      "Epoch 128, Train Loss: 0.027252396568655968, Train RMSE: 0.16502448916435242, Val RMSE: 0.7205483913421631 time: 0.4677419662475586\n",
      "\n",
      "Epoch 00129: CosineAnnealing lr to 0.00016407739170712157.\n",
      "Epoch 129, Train Loss: 0.04142655059695244, Train RMSE: 0.21276259422302246, Val RMSE: 0.7310915589332581 time: 0.47419095039367676\n",
      "\n",
      "Epoch 00130: CosineAnnealing lr to 4.279207274241037e-05.\n",
      "Restart at epoch 00130\n",
      "Epoch 130, Train Loss: 0.06162174418568611, Train RMSE: 0.2466164380311966, Val RMSE: 0.6500231623649597 time: 0.5021300315856934\n",
      "\n",
      "Epoch 00131: CosineAnnealing lr to 0.0016233301862865123.\n",
      "Epoch 131, Train Loss: 0.05390141159296036, Train RMSE: 0.22640158236026764, Val RMSE: 0.6592612862586975 time: 0.47251033782958984\n",
      "\n",
      "Epoch 00132: CosineAnnealing lr to 0.0015836289407683151.\n",
      "Epoch 132, Train Loss: 0.06343357264995575, Train RMSE: 0.2635689675807953, Val RMSE: 0.6958467960357666 time: 0.4728660583496094\n",
      "\n",
      "Epoch 00133: CosineAnnealing lr to 0.0014684114387398875.\n",
      "Epoch 133, Train Loss: 0.09203782677650452, Train RMSE: 0.30485400557518005, Val RMSE: 0.6459105610847473 time: 0.4708998203277588\n",
      "\n",
      "Epoch 00134: CosineAnnealing lr to 0.0012889559720673125.\n",
      "Epoch 134, Train Loss: 0.05751604586839676, Train RMSE: 0.2377670854330063, Val RMSE: 0.7348954677581787 time: 0.4682619571685791\n",
      "\n",
      "Epoch 00135: CosineAnnealing lr to 0.0010628288921682594.\n",
      "Epoch 135, Train Loss: 0.05560985207557678, Train RMSE: 0.246424600481987, Val RMSE: 0.7268369197845459 time: 0.47381591796875\n",
      "\n",
      "Epoch 00136: CosineAnnealing lr to 0.0008121650931432562.\n",
      "Epoch 136, Train Loss: 0.05733352154493332, Train RMSE: 0.2367931604385376, Val RMSE: 0.6446794271469116 time: 0.4747138023376465\n",
      "\n",
      "Epoch 00137: CosineAnnealing lr to 0.000561501294118253.\n",
      "Epoch 137, Train Loss: 0.03249824792146683, Train RMSE: 0.18531692028045654, Val RMSE: 0.6583276391029358 time: 0.47376012802124023\n",
      "\n",
      "Epoch 00138: CosineAnnealing lr to 0.0003353742142192.\n",
      "Epoch 138, Train Loss: 0.035537637770175934, Train RMSE: 0.1936083287000656, Val RMSE: 0.6395633816719055 time: 0.47000789642333984\n",
      "\n",
      "Epoch 00139: CosineAnnealing lr to 0.00015591874754662487.\n",
      "Epoch 139, Train Loss: 0.02388109639286995, Train RMSE: 0.15462934970855713, Val RMSE: 0.6708765625953674 time: 0.47168588638305664\n",
      "\n",
      "Epoch 00140: CosineAnnealing lr to 4.0701245518197226e-05.\n",
      "Restart at epoch 00140\n",
      "Epoch 140, Train Loss: 0.02010423317551613, Train RMSE: 0.1472449004650116, Val RMSE: 0.6763520240783691 time: 0.49919915199279785\n",
      "\n",
      "Epoch 00141: CosineAnnealing lr to 0.0015421636769721867.\n",
      "Epoch 141, Train Loss: 0.019400309771299362, Train RMSE: 0.13884928822517395, Val RMSE: 0.6549699902534485 time: 0.4695439338684082\n",
      "\n",
      "Epoch 00142: CosineAnnealing lr to 0.001504448717316992.\n",
      "Epoch 142, Train Loss: 0.014029532670974731, Train RMSE: 0.1224326565861702, Val RMSE: 0.653913676738739 time: 0.4732820987701416\n",
      "\n",
      "Epoch 00143: CosineAnnealing lr to 0.0013949956413780338.\n",
      "Epoch 143, Train Loss: 0.016017261892557144, Train RMSE: 0.12957173585891724, Val RMSE: 0.6418300271034241 time: 0.47768211364746094\n",
      "\n",
      "Epoch 00144: CosineAnnealing lr to 0.0012245184788326395.\n",
      "Epoch 144, Train Loss: 0.011516296304762363, Train RMSE: 0.1091897264122963, Val RMSE: 0.6607943177223206 time: 0.4696390628814697\n",
      "\n",
      "Epoch 00145: CosineAnnealing lr to 0.0010097047221349872.\n",
      "Epoch 145, Train Loss: 0.010213054716587067, Train RMSE: 0.10530167073011398, Val RMSE: 0.6674908995628357 time: 0.47010207176208496\n",
      "\n",
      "Epoch 00146: CosineAnnealing lr to 0.0007715818384860934.\n",
      "Epoch 146, Train Loss: 0.010407241992652416, Train RMSE: 0.10380351543426514, Val RMSE: 0.6514805555343628 time: 0.47566699981689453\n",
      "\n",
      "Epoch 00147: CosineAnnealing lr to 0.0005334589548371998.\n",
      "Epoch 147, Train Loss: 0.008186805993318558, Train RMSE: 0.09239109605550766, Val RMSE: 0.6458642482757568 time: 0.4799618721008301\n",
      "\n",
      "Epoch 00148: CosineAnnealing lr to 0.0003186451981395473.\n",
      "Epoch 148, Train Loss: 0.00910398829728365, Train RMSE: 0.09983301162719727, Val RMSE: 0.6439192891120911 time: 0.47652387619018555\n",
      "\n",
      "Epoch 00149: CosineAnnealing lr to 0.00014816803559415299.\n",
      "Epoch 149, Train Loss: 0.008879011496901512, Train RMSE: 0.09628134965896606, Val RMSE: 0.6483656764030457 time: 0.4773099422454834\n",
      "\n",
      "Epoch 00150: CosineAnnealing lr to 3.871495965519474e-05.\n",
      "Restart at epoch 00150\n",
      "Epoch 150, Train Loss: 0.006671345327049494, Train RMSE: 0.08381852507591248, Val RMSE: 0.6631374955177307 time: 0.495805025100708\n",
      "\n",
      "Epoch 00151: CosineAnnealing lr to 0.0014650554931235772.\n",
      "Epoch 151, Train Loss: 0.007293588016182184, Train RMSE: 0.08925130218267441, Val RMSE: 0.6650228500366211 time: 0.4760160446166992\n",
      "\n",
      "Epoch 00152: CosineAnnealing lr to 0.001429227505038235.\n",
      "Epoch 152, Train Loss: 0.0076877824030816555, Train RMSE: 0.0889299139380455, Val RMSE: 0.6504438519477844 time: 0.4729928970336914\n",
      "\n",
      "Epoch 00153: CosineAnnealing lr to 0.0013252506338842726.\n",
      "Epoch 153, Train Loss: 0.00655228178948164, Train RMSE: 0.0820375382900238, Val RMSE: 0.641612708568573 time: 0.471904993057251\n",
      "\n",
      "Epoch 00154: CosineAnnealing lr to 0.0011633028602597.\n",
      "Epoch 154, Train Loss: 0.0076455846428871155, Train RMSE: 0.0914962887763977, Val RMSE: 0.6413131356239319 time: 0.47524309158325195\n",
      "\n",
      "Epoch 00155: CosineAnnealing lr to 0.0009592367606033784.\n",
      "Epoch 155, Train Loss: 0.009291605092585087, Train RMSE: 0.09901800006628036, Val RMSE: 0.6465638279914856 time: 0.47698211669921875\n",
      "\n",
      "Epoch 00156: CosineAnnealing lr to 0.0007330277465617887.\n",
      "Epoch 156, Train Loss: 0.007970589213073254, Train RMSE: 0.08961117267608643, Val RMSE: 0.6623456478118896 time: 0.4685680866241455\n",
      "\n",
      "Epoch 00157: CosineAnnealing lr to 0.000506818732520199.\n",
      "Epoch 157, Train Loss: 0.0077894097194075584, Train RMSE: 0.09114555269479752, Val RMSE: 0.6759943962097168 time: 0.47123193740844727\n",
      "\n",
      "Epoch 00158: CosineAnnealing lr to 0.00030275263286387723.\n",
      "Epoch 158, Train Loss: 0.010760256089270115, Train RMSE: 0.10774996131658554, Val RMSE: 0.6664585471153259 time: 0.47361278533935547\n",
      "\n",
      "Epoch 00159: CosineAnnealing lr to 0.0001408048592393047.\n",
      "Epoch 159, Train Loss: 0.011754778213799, Train RMSE: 0.10778003185987473, Val RMSE: 0.6440181732177734 time: 0.47547292709350586\n",
      "\n",
      "Epoch 00160: CosineAnnealing lr to 3.682798808534238e-05.\n",
      "Restart at epoch 00160\n",
      "Epoch 160, Train Loss: 0.010581334121525288, Train RMSE: 0.10247257351875305, Val RMSE: 0.6416033506393433 time: 0.4942803382873535\n",
      "\n",
      "Epoch 00161: CosineAnnealing lr to 0.0013918027184673983.\n",
      "Epoch 161, Train Loss: 0.015582522377371788, Train RMSE: 0.130801722407341, Val RMSE: 0.649773120880127 time: 0.4748549461364746\n",
      "\n",
      "Epoch 00162: CosineAnnealing lr to 0.0013577673533734158.\n",
      "Epoch 162, Train Loss: 0.022559605538845062, Train RMSE: 0.15285414457321167, Val RMSE: 0.6478232741355896 time: 0.4757423400878906\n",
      "\n",
      "Epoch 00163: CosineAnnealing lr to 0.0012589928767651995.\n",
      "Epoch 163, Train Loss: 0.02005661651492119, Train RMSE: 0.13804762065410614, Val RMSE: 0.6781404614448547 time: 0.4753248691558838\n",
      "\n",
      "Epoch 00164: CosineAnnealing lr to 0.0011051480226154078.\n",
      "Epoch 164, Train Loss: 0.02130497433245182, Train RMSE: 0.15018421411514282, Val RMSE: 0.7240626215934753 time: 0.4777090549468994\n",
      "\n",
      "Epoch 00165: CosineAnnealing lr to 0.0009112921971483501.\n",
      "Epoch 165, Train Loss: 0.03605983406305313, Train RMSE: 0.19633089005947113, Val RMSE: 0.6959161162376404 time: 0.4750480651855469\n",
      "\n",
      "Epoch 00166: CosineAnnealing lr to 0.0006964013592336992.\n",
      "Epoch 166, Train Loss: 0.0410514697432518, Train RMSE: 0.19718864560127258, Val RMSE: 0.6389395594596863 time: 0.47431182861328125\n",
      "\n",
      "Epoch 00167: CosineAnnealing lr to 0.00048151052131904847.\n",
      "Epoch 167, Train Loss: 0.033993348479270935, Train RMSE: 0.18541310727596283, Val RMSE: 0.671728789806366 time: 0.4737730026245117\n",
      "\n",
      "Epoch 00168: CosineAnnealing lr to 0.0002876546958519907.\n",
      "Epoch 168, Train Loss: 0.05537145212292671, Train RMSE: 0.24464906752109528, Val RMSE: 0.6714817881584167 time: 0.47762203216552734\n",
      "\n",
      "Epoch 00169: CosineAnnealing lr to 0.00013380984170219883.\n",
      "Epoch 169, Train Loss: 0.06634465605020523, Train RMSE: 0.2545609176158905, Val RMSE: 0.6664202213287354 time: 0.4713568687438965\n",
      "\n",
      "Epoch 00170: CosineAnnealing lr to 3.503536509398264e-05.\n",
      "Restart at epoch 00170\n",
      "Epoch 170, Train Loss: 0.04514235630631447, Train RMSE: 0.21261923015117645, Val RMSE: 0.7504352927207947 time: 0.49591803550720215\n",
      "\n",
      "Epoch 00171: CosineAnnealing lr to 0.0013222125825440283.\n",
      "Epoch 171, Train Loss: 0.054980773478746414, Train RMSE: 0.2434639036655426, Val RMSE: 0.7224004864692688 time: 0.4757988452911377\n",
      "\n",
      "Epoch 00172: CosineAnnealing lr to 0.0012898802092918375.\n"
     ]
    }
   ],
   "source": [
    "train_model(train_data, val_data, epochs=epochs, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ef7d9-3850-4183-97ca-af544934d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42372236-04ce-4537-bd58-3a9dadb1c817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e7613-6f75-48b7-9567-e96ee85d5b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
